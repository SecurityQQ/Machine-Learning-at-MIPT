## Results
Лучшее качество кросс-энтропии для GoogLENET: $2.04704605341$ (1400 эпох)
Лучшее качество кросс-энтропии для RESNET50: $2.57693591595$ (114 эпох)

### Получившиеся подписи GoogLENET:
- a small dog laying in a grassy field
- a small cat is sitting on the green grass
- a very cute small furry animal in the grass

### RESNET50:
- a dog is sitting on the floor with a dog
Больше деталей смотрите в ноутбуках Malyshev_BEST_RESULTS_LENET.ipynb и Malyshev_WITH_RESNET.ipynb 

<h1 align="center">Теор. вопросы</h1> 

**Вопрос 1**: Можно ли использовать сверточные сети для классификации текстов? Если нет обоснуйте :D, если да то как? как решить проблему с произвольной длинной входа?

Можно делать onehot encoding для букв, затем делать свертки на последовательности фиксированной длины (это и будет решение для неогр. длины входа). Проблема в том, что мы не учитываем последовательность букв. Можно использовать n-граммы, но это сильно увеличивает и без того немаленький датасет + по большому счету это эвристика/костыль.

**Вопрос 2**: Чем LSTM лучше/хуже чем обычная RNN?

Ключевая особенность LSTM в том, что она может дольше сохранять информацию о прошлом, как следствие, лучше выучивает грамматику. Из минусов -- градиенты могут 'взорваться'.

**Вопрос 3**:  Выпишите производную $\frac{d c_{n+1}}{d c_{k}}$ для LSTM http://colah.github.io/posts/2015-08-Understanding-LSTMs/, объясните формулу, когда производная затухает, когда взрывается?

$\frac{d c_{n+1}}{d c_{k}} = \frac{d c_{n+1}}{d c_{n}}\frac{d c_{n}}{d c_{k}}$


**Вопрос 4**: Зачем нужен TBPTT почему BPTT плох?

У BPTT затухает градиент. TBPTT борется с этой проблемой: запуская BPTT определенное количество интераций каждое фиксированное количество шагов.


**Вопрос 5**: Как комбинировать рекуррентные и сверточные сети, а главное зачем? Приведите несколько примеров реальных задач.

По картинкам генерировать описание, звуки. Можно делать image captioning для видео и делать style transfer, генерировать новые названия получившимся объектам и т.п.

**Вопрос 6**: Объясните интуицию выбора размера эмбединг слоя? почему это опасное место?

Взаимосвязи слов могут не сохранится если взять очень большой embeding size
