{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1> Deep Learning Seminar 1</h1>\n",
    "\n",
    "Credit cs231n.stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2 align=\"center\">What was at Lecture?</h2>\n",
    "\n",
    "- Image Classification \n",
    "\n",
    "<img src=\"img/img-clf.png\" width=\"600\">\n",
    "\n",
    "- Linear Models (Что делает линейная модель простым языком)\n",
    "\n",
    "<img src=\"img/lm.png\" width=\"600\">\n",
    "<img src=\"img/lm-int.png\" width=\"600\">\n",
    "\n",
    "- Fully Connected Neural Nets\n",
    "\n",
    "<img src=\"img/fc-net.png\" width=\"600\">\n",
    "\n",
    "- Convolution Neural Nets (Какая мотивация при переходе к сверткам?)\n",
    "\n",
    "<img src=\"img/conv.png\" width=\"600\">\n",
    "\n",
    "- Зачем нужен backprop? \n",
    "\n",
    "<img src=\"img/bp.png\" width=\"600\">\n",
    "\n",
    "The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "\n",
    "- Что нужно накрутить на SGD чтобы получить хорошие методы стах оптимизации?\n",
    "\n",
    "<img src=\"img/adam.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2 align=\"center\">BackProp and Optimizers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import check_grad\n",
    "from gradient_check import eval_numerical_gradient_array\n",
    "\n",
    "def rel_error(x, y):\n",
    "      return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Grad Check</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"img/gc.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Softmax Loss Layer</h3>\n",
    "<img src=\"img/loss.png\" width=\"300\">\n",
    "<img src=\"img/log.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    " def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dx = np.zeros_like(x)\n",
    "    for i in range(x.shape[0]):\n",
    "        loss += (np.log(np.exp(x[i]).sum()) - x[i, y[i]]) / x.shape[0]\n",
    "        dx[i] = 1.0 / (np.exp(x[i]).sum()) * np.exp(x[i])\n",
    "        dx[i, y[i]] -= 1\n",
    "    dx /= x.shape[0]\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 3, 10)\n",
    "dx = lambda x: softmax_loss(x.reshape((10, 3)), y)[1].reshape(-1)\n",
    "loss = lambda x: softmax_loss(x.reshape((10, 3)), y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is a scalar\n",
      "1.17462529299\n"
     ]
    }
   ],
   "source": [
    "print 'loss is a scalar\\n', loss(np.random.random((10, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient is a matrix with shape 10x3\n",
      "[-0.06120295  0.02624532  0.03495762  0.03726306 -0.07759992  0.04033685\n",
      "  0.04546296 -0.07432668  0.02886372  0.02392254 -0.06226204  0.0383395\n",
      " -0.061315    0.02780949  0.0335055  -0.06514048  0.04129348  0.02384699\n",
      "  0.02396579 -0.05297713  0.02901135 -0.05823053  0.0245769   0.03365363\n",
      "  0.03797311  0.0378852  -0.07585831  0.04082392  0.03182703 -0.07265095]\n"
     ]
    }
   ],
   "source": [
    "print 'gradient is a matrix with shape 10x3\\n', dx(np.random.random((10, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference should be ~10e-8 3.28722963313e-08\n"
     ]
    }
   ],
   "source": [
    "print 'difference should be ~10e-8', check_grad(loss, dx, np.random.random((10, 3)).reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Dense Layer</h3>\n",
    "<img src=\"img/lin.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "    # will need to reshape the input into rows.                                 #\n",
    "    #############################################################################\n",
    "    out = x.reshape([x.shape[0], -1]).dot(w) + b\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.76985004799e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print 'Testing affine_forward function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine backward pass.                                 #\n",
    "    #############################################################################\n",
    "    dx = dout.dot(w.T).reshape(x.shape)\n",
    "    dw = dout.T.dot(x.reshape([x.shape[0], -1])).T\n",
    "    db = dout.sum(axis=0)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  2.20074145993e-09\n",
      "dw error:  1.09092420167e-10\n",
      "db error:  3.05963131672e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print 'Testing affine_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>ReLu Layer</h3>\n",
    "\n",
    "$$ReLu(x) = max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                    #\n",
    "    #############################################################################\n",
    "    out = np.maximum(x, 0)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.99999979802e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print 'Testing relu_forward function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                   #\n",
    "    #############################################################################\n",
    "    dx = np.multiply(dout, x > 0)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.27561299275e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print 'Testing relu_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Two Layer Fully Connected Neural Net with SGD</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10bb064d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b734050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.imshow(X[5].reshape((8, 8)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (100,64) and (2,) not aligned: 64 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-470b8aa270fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# ------------ Train -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Forward Pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Dense Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# ReLu Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mout3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Dense Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-45a524ce5858>\u001b[0m in \u001b[0;36maffine_forward\u001b[0;34m(x, w, b)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# will need to reshape the input into rows.                                 #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m#############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#                             END OF YOUR CODE                              #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (100,64) and (2,) not aligned: 64 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "W1, b1 = np.random.normal((0, 0.01), (64, 100)), np.random.random(100)\n",
    "W2, b2 = np.random.random((100, 10)), np.random.random(10)\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "for i in range(50000):\n",
    "    batch_index = np.random.randint(0, X_train.shape[0], 100)\n",
    "    batch_X, batch_y = X_train[batch_index], y_train[batch_index]\n",
    "    \n",
    "    # ------------ Train ----------------- \n",
    "    # Forward Pass\n",
    "    out1, cache1 = affine_forward(batch_X, W1, b1) # Dense Layer\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2,    W2, b2) # Dense Layer \n",
    "    tr_loss, dout3 = softmax_loss(out3, batch_y)      # Loss Layer \n",
    "    \n",
    "    # Backward Pass\n",
    "    dout2, dw2, db2 = affine_backward(dout3, cache3)\n",
    "    dout1 = relu_backward(dout2, cache2)\n",
    "    _, dw0, db0 = affine_backward(dout1, cache1)\n",
    "    # Updates\n",
    "    W2 -= lr * dw2\n",
    "    W1 -= lr * dw1\n",
    "    b1 -= lr * db1\n",
    "    b2 -= lr * db2\n",
    "    \n",
    "    # ------------ Test ----------------- \n",
    "    # Forward Pass\n",
    "    out1, cache1 = affine_forward(X_test, W1, b1) # Dense Layer\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2, W2, b2) # Dense Layer \n",
    "    te_loss, dout3 = softmax_loss(out3, batch_y)      # Loss Layer \n",
    "    # Predict\n",
    "    y_pred = out3.argmax(axis=1)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print 'epoch %s:' % i, \n",
    "        print '\\t tr_loss %.2f' % tr_loss,\n",
    "        print '\\t te_loss %.2f' % te_loss,\n",
    "        print '\\t te_acc %s' % accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2 align=\"center\">What is the challenge? </h2>\n",
    "\n",
    "You will see in Assignment 1:\n",
    "- more layers and architectures (Dropout, Convolution, Pooling)\n",
    "- optimization (Momentum, Adam)\n",
    "- weight initialization \n",
    "- data augmentation \n",
    "- ...\n",
    "\n",
    "<img src=\"img/rw.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
