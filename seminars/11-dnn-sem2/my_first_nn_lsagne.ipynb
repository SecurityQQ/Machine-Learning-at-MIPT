{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 align=\"center\">Lecture</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Активации из сетей для классификации, это хорошие признаки для изображений\n",
    "\n",
    "<img src=\"img/act.png\" width=\"800\">\n",
    "\n",
    "\n",
    "## Современные архитектуры очень глубокие , самые модные \n",
    "\n",
    "### VGG (стандартная архитектура, без наворотов)\n",
    "\n",
    "<img src=\"img/vgg.png\" width=\"600\">\n",
    "\n",
    "### ResNet (Shortcut + Batch Normalization)\n",
    " \n",
    "<img src=\"img/resnet.png\" width=\"800\">\n",
    " \n",
    "### GoogleNet (Много раз предсказываем классы на разных уровнях сети)\n",
    "\n",
    " \n",
    "<img src=\"img/gln.png\" width=\"800\">\n",
    "\n",
    "\n",
    "## Чем глубже слой тем более высокоуровневые признаки он детектирует\n",
    "\n",
    "<img src=\"img/feat.png\" width=\"800\">\n",
    "\n",
    "## На практике гораздо проще дообучать уже обученные сети (Fine-Tuning)\n",
    "\n",
    "<img src=\"img/ft.jpg\" width=\"600\">\n",
    "\n",
    "## Dark Magic \n",
    "\n",
    "<img src=\"img/dm.png\" width=\"600\">\n",
    "\n",
    "# Сегодня Theano and Lasagne :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 align=\"center\">Theano</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```bash\n",
    "pip install -U https://github.com/Theano/Theano/archive/master.zip\n",
    "pip install -U https://github.com/Lasagne/Lasagne/archive/master.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Разминка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### будущий параметр функции -- символьная переменная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N = T.scalar('a dimension', dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### рецепт получения квадрата -- орперации над символьными переменным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result = T.power(N, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### theano.grad(cost, wrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grad_result = theano.grad(result, N) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### компиляция функции \"получения квадрата\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sq_function = theano.function(inputs=[N], outputs=result)\n",
    "gr_function = theano.function(inputs=[N], outputs=grad_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### применение функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Заводим np.array x\n",
    "xv = np.arange(-10, 10)\n",
    "\n",
    "# Применяем функцию к каждому x\n",
    "val = map(float, [sq_function(x) for x in xv])\n",
    "\n",
    "# Посичтаем градиент в кажой точке\n",
    "grad = map(float, [gr_function(x) for x in xv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Что мы увидим если нарисуем функцию и градиент?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11021d190>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBAuQhK+EEK4CEn4QgjhIiThCyGEi/h/3cVMHQw86eIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11588b1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.plot(xv, val, label='x*x')\n",
    "pylab.plot(xv, grad, label='d x*x / dx')\n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Как оно работает?\n",
    "* почти всё, что есть в numpy есть в theano tensor и называется так же: `np.mean -> T.mean` и так далее...\n",
    "* `theano.function` умеет за одно обновлять `shared` переменные по рецепту в `updates`\n",
    "* Переменные нужно хранить в `shared` переменных, их можно менять после компиляции `theano.shared(np.ones(10))`\n",
    "\n",
    " \n",
    "Ничего не понятно? Сейчас исправим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Теперь сам, LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X_data, y_data = datasets.load_digits(2, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y метки классов 0 или 1 [форма - (360,)]: [0 1 0 1 0 1 0 0 1 1]\n",
      "X цифорки вытянутые в вектор [форма - (360, 64)]:\n",
      "[[  0.   0.   5.  13.   9.   1.   0.   0.]\n",
      " [  0.   0.  13.  15.  10.  15.   5.   0.]\n",
      " [  0.   3.  15.   2.   0.  11.   8.   0.]\n",
      " [  0.   4.  12.   0.   0.   8.   8.   0.]\n",
      " [  0.   5.   8.   0.   0.   9.   8.   0.]\n",
      " [  0.   4.  11.   0.   1.  12.   7.   0.]\n",
      " [  0.   2.  14.   5.  10.  12.   0.   0.]\n",
      " [  0.   0.   6.  13.  10.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print 'y метки классов 0 или 1 [форма - %s]:' % (str(y_data.shape)),y_data[:10]\n",
    "print 'X цифорки вытянутые в вектор [форма - %s]:' % (str(X_data.shape))\n",
    "print X_data[0].reshape((8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# переменные и входы\n",
    "W = theano.shared(np.random.normal(0, 1, 64))\n",
    "X = T.fmatrix()\n",
    "y = T.fvector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predicted_y = 1 / (1 + T.exp(-T.dot(X, W)))\n",
    "loss = ((y - predicted_y) ** 2).mean()\n",
    "grad = T.grad(loss, W)\n",
    "# {W: <новое значение весов после шага градиентного спуска>}\n",
    "learning_rate = 0.05\n",
    "updates = {W: W - learning_rate * grad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_function = theano.function([X, y], loss, updates=updates, allow_input_downcast=True)\n",
    "predict_function = theano.function([X], predicted_y, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss at iter 0:0.2690  train auc: 0.884917582418  test auc: 0.849702380952\n",
      " loss at iter 1:0.2645  train auc: 0.89282967033  test auc: 0.850198412698\n",
      " loss at iter 2:0.2604  train auc: 0.900961538462  test auc: 0.848214285714\n",
      " loss at iter 3:0.2547  train auc: 0.908434065934  test auc: 0.855902777778\n",
      " loss at iter 4:0.2448  train auc: 0.91989010989  test auc: 0.876736111111\n",
      " loss at iter 5:0.2304  train auc: 0.920879120879  test auc: 0.902281746032\n",
      " loss at iter 6:0.2213  train auc: 0.921428571429  test auc: 0.921626984127\n",
      " loss at iter 7:0.2152  train auc: 0.925054945055  test auc: 0.933531746032\n",
      " loss at iter 8:0.2075  train auc: 0.932005494505  test auc: 0.934027777778\n",
      " loss at iter 9:0.1947  train auc: 0.942527472527  test auc: 0.946428571429\n",
      " loss at iter 10:0.1810  train auc: 0.944230769231  test auc: 0.948908730159\n",
      " loss at iter 11:0.1659  train auc: 0.945989010989  test auc: 0.951884920635\n",
      " loss at iter 12:0.1499  train auc: 0.947582417582  test auc: 0.960069444444\n",
      " loss at iter 13:0.1405  train auc: 0.949010989011  test auc: 0.963541666667\n",
      " loss at iter 14:0.1352  train auc: 0.95010989011  test auc: 0.966021825397\n",
      " loss at iter 15:0.1299  train auc: 0.957362637363  test auc: 0.968005952381\n",
      " loss at iter 16:0.1215  train auc: 0.961318681319  test auc: 0.968998015873\n",
      " loss at iter 17:0.1059  train auc: 0.965  test auc: 0.971974206349\n",
      " loss at iter 18:0.0930  train auc: 0.966098901099  test auc: 0.97371031746\n",
      " loss at iter 19:0.0871  train auc: 0.966758241758  test auc: 0.974206349206\n",
      " loss at iter 20:0.0833  train auc: 0.967692307692  test auc: 0.973958333333\n",
      " loss at iter 21:0.0806  train auc: 0.968681318681  test auc: 0.975942460317\n",
      " loss at iter 22:0.0786  train auc: 0.96956043956  test auc: 0.983630952381\n",
      " loss at iter 23:0.0769  train auc: 0.970164835165  test auc: 0.985119047619\n",
      " loss at iter 24:0.0755  train auc: 0.970714285714  test auc: 0.986607142857\n",
      " loss at iter 25:0.0744  train auc: 0.971153846154  test auc: 0.987599206349\n",
      " loss at iter 26:0.0735  train auc: 0.971978021978  test auc: 0.987599206349\n",
      " loss at iter 27:0.0726  train auc: 0.972417582418  test auc: 0.987599206349\n",
      " loss at iter 28:0.0718  train auc: 0.972912087912  test auc: 0.988095238095\n",
      " loss at iter 29:0.0710  train auc: 0.973571428571  test auc: 0.988095238095\n",
      " loss at iter 30:0.0703  train auc: 0.973846153846  test auc: 0.988591269841\n",
      " loss at iter 31:0.0695  train auc: 0.974615384615  test auc: 0.990079365079\n",
      " loss at iter 32:0.0688  train auc: 0.975274725275  test auc: 0.990575396825\n",
      " loss at iter 33:0.0680  train auc: 0.975714285714  test auc: 0.991071428571\n",
      " loss at iter 34:0.0672  train auc: 0.975934065934  test auc: 0.991071428571\n",
      " loss at iter 35:0.0664  train auc: 0.976373626374  test auc: 0.991071428571\n",
      " loss at iter 36:0.0656  train auc: 0.976703296703  test auc: 0.991071428571\n",
      " loss at iter 37:0.0649  train auc: 0.977197802198  test auc: 0.992063492063\n",
      " loss at iter 38:0.0642  train auc: 0.977637362637  test auc: 0.992063492063\n",
      " loss at iter 39:0.0635  train auc: 0.977912087912  test auc: 0.99255952381\n",
      " loss at iter 40:0.0629  train auc: 0.978461538462  test auc: 0.99255952381\n",
      " loss at iter 41:0.0624  train auc: 0.978901098901  test auc: 0.99255952381\n",
      " loss at iter 42:0.0619  train auc: 0.979175824176  test auc: 0.99255952381\n",
      " loss at iter 43:0.0613  train auc: 0.979340659341  test auc: 0.993055555556\n",
      " loss at iter 44:0.0608  train auc: 0.97967032967  test auc: 0.993055555556\n",
      " loss at iter 45:0.0603  train auc: 0.979945054945  test auc: 0.993055555556\n",
      " loss at iter 46:0.0597  train auc: 0.980164835165  test auc: 0.994047619048\n",
      " loss at iter 47:0.0592  train auc: 0.980494505495  test auc: 0.994047619048\n",
      " loss at iter 48:0.0585  train auc: 0.980769230769  test auc: 0.994047619048\n",
      " loss at iter 49:0.0579  train auc: 0.981043956044  test auc: 0.994543650794\n",
      " loss at iter 50:0.0572  train auc: 0.983791208791  test auc: 0.994543650794\n",
      " loss at iter 51:0.0565  train auc: 0.984175824176  test auc: 0.994543650794\n",
      " loss at iter 52:0.0559  train auc: 0.984450549451  test auc: 0.994543650794\n",
      " loss at iter 53:0.0553  train auc: 0.984917582418  test auc: 0.994543650794\n",
      " loss at iter 54:0.0548  train auc: 0.985137362637  test auc: 0.994543650794\n",
      " loss at iter 55:0.0544  train auc: 0.985302197802  test auc: 0.994543650794\n",
      " loss at iter 56:0.0540  train auc: 0.985467032967  test auc: 0.99503968254\n",
      " loss at iter 57:0.0536  train auc: 0.985714285714  test auc: 0.99503968254\n",
      " loss at iter 58:0.0533  train auc: 0.985714285714  test auc: 0.99503968254\n",
      " loss at iter 59:0.0530  train auc: 0.985989010989  test auc: 0.99503968254\n",
      " loss at iter 60:0.0527  train auc: 0.986098901099  test auc: 0.99503968254\n",
      " loss at iter 61:0.0524  train auc: 0.986153846154  test auc: 0.995535714286\n",
      " loss at iter 62:0.0521  train auc: 0.986208791209  test auc: 0.995535714286\n",
      " loss at iter 63:0.0519  train auc: 0.986318681319  test auc: 0.995535714286\n",
      " loss at iter 64:0.0516  train auc: 0.986565934066  test auc: 0.995535714286\n",
      " loss at iter 65:0.0513  train auc: 0.986785714286  test auc: 0.995535714286\n",
      " loss at iter 66:0.0510  train auc: 0.986840659341  test auc: 0.995535714286\n",
      " loss at iter 67:0.0507  train auc: 0.987115384615  test auc: 0.995535714286\n",
      " loss at iter 68:0.0503  train auc: 0.98739010989  test auc: 0.995535714286\n",
      " loss at iter 69:0.0498  train auc: 0.987582417582  test auc: 0.995535714286\n",
      " loss at iter 70:0.0493  train auc: 0.987664835165  test auc: 0.995535714286\n",
      " loss at iter 71:0.0487  train auc: 0.988214285714  test auc: 0.995535714286\n",
      " loss at iter 72:0.0481  train auc: 0.988406593407  test auc: 0.995535714286\n",
      " loss at iter 73:0.0474  train auc: 0.988516483516  test auc: 0.995535714286\n",
      " loss at iter 74:0.0469  train auc: 0.988571428571  test auc: 0.995535714286\n",
      " loss at iter 75:0.0464  train auc: 0.988681318681  test auc: 0.995535714286\n",
      " loss at iter 76:0.0460  train auc: 0.988846153846  test auc: 0.995535714286\n",
      " loss at iter 77:0.0456  train auc: 0.989010989011  test auc: 0.995535714286\n",
      " loss at iter 78:0.0452  train auc: 0.989340659341  test auc: 0.995535714286\n",
      " loss at iter 79:0.0449  train auc: 0.989587912088  test auc: 0.996031746032\n",
      " loss at iter 80:0.0446  train auc: 0.989697802198  test auc: 0.996031746032\n",
      " loss at iter 81:0.0442  train auc: 0.986813186813  test auc: 0.996031746032\n",
      " loss at iter 82:0.0438  train auc: 0.986813186813  test auc: 0.996031746032\n",
      " loss at iter 83:0.0434  train auc: 0.986868131868  test auc: 0.996031746032\n",
      " loss at iter 84:0.0428  train auc: 0.987032967033  test auc: 0.996031746032\n",
      " loss at iter 85:0.0422  train auc: 0.987197802198  test auc: 0.996031746032\n",
      " loss at iter 86:0.0413  train auc: 0.987362637363  test auc: 0.996527777778\n",
      " loss at iter 87:0.0403  train auc: 0.987582417582  test auc: 0.996527777778\n",
      " loss at iter 88:0.0392  train auc: 0.987637362637  test auc: 0.996527777778\n",
      " loss at iter 89:0.0382  train auc: 0.987802197802  test auc: 0.996527777778\n",
      " loss at iter 90:0.0371  train auc: 0.987912087912  test auc: 0.996527777778\n",
      " loss at iter 91:0.0362  train auc: 0.988076923077  test auc: 0.996527777778\n",
      " loss at iter 92:0.0353  train auc: 0.988131868132  test auc: 0.996527777778\n",
      " loss at iter 93:0.0344  train auc: 0.988406593407  test auc: 0.996527777778\n",
      " loss at iter 94:0.0334  train auc: 0.988626373626  test auc: 0.997023809524\n",
      " loss at iter 95:0.0325  train auc: 0.988736263736  test auc: 0.997023809524\n",
      " loss at iter 96:0.0315  train auc: 0.988846153846  test auc: 0.997023809524\n",
      " loss at iter 97:0.0306  train auc: 0.988901098901  test auc: 0.997023809524\n",
      " loss at iter 98:0.0297  train auc: 0.989010989011  test auc: 0.997023809524\n",
      " loss at iter 99:0.0290  train auc: 0.989010989011  test auc: 0.997023809524\n",
      " loss at iter 100:0.0283  train auc: 0.989065934066  test auc: 0.997023809524\n",
      " loss at iter 101:0.0278  train auc: 0.989230769231  test auc: 0.99751984127\n",
      " loss at iter 102:0.0273  train auc: 0.989230769231  test auc: 0.998015873016\n",
      " loss at iter 103:0.0268  train auc: 0.989340659341  test auc: 0.998015873016\n",
      " loss at iter 104:0.0264  train auc: 0.989340659341  test auc: 0.998015873016\n",
      " loss at iter 105:0.0260  train auc: 0.989395604396  test auc: 0.998015873016\n",
      " loss at iter 106:0.0257  train auc: 0.989395604396  test auc: 0.998015873016\n",
      " loss at iter 107:0.0253  train auc: 0.989395604396  test auc: 0.998015873016\n",
      " loss at iter 108:0.0250  train auc: 0.989505494505  test auc: 0.998015873016\n",
      " loss at iter 109:0.0248  train auc: 0.98956043956  test auc: 0.998511904762\n",
      " loss at iter 110:0.0245  train auc: 0.989615384615  test auc: 0.998511904762\n",
      " loss at iter 111:0.0243  train auc: 0.989615384615  test auc: 0.998511904762\n",
      " loss at iter 112:0.0241  train auc: 0.989615384615  test auc: 0.998511904762\n",
      " loss at iter 113:0.0239  train auc: 0.989615384615  test auc: 0.999007936508\n",
      " loss at iter 114:0.0237  train auc: 0.989615384615  test auc: 0.999007936508\n",
      " loss at iter 115:0.0235  train auc: 0.98967032967  test auc: 0.999007936508\n",
      " loss at iter 116:0.0234  train auc: 0.98967032967  test auc: 0.999007936508\n",
      " loss at iter 117:0.0233  train auc: 0.989725274725  test auc: 0.999007936508\n",
      " loss at iter 118:0.0231  train auc: 0.989835164835  test auc: 0.999007936508\n",
      " loss at iter 119:0.0230  train auc: 0.989835164835  test auc: 0.999007936508\n",
      " loss at iter 120:0.0229  train auc: 0.989835164835  test auc: 0.999007936508\n",
      " loss at iter 121:0.0228  train auc: 0.989835164835  test auc: 0.999007936508\n",
      " loss at iter 122:0.0226  train auc: 0.989835164835  test auc: 0.999007936508\n",
      " loss at iter 123:0.0225  train auc: 0.989835164835  test auc: 0.999007936508\n",
      " loss at iter 124:0.0224  train auc: 0.993186813187  test auc: 0.999007936508\n",
      " loss at iter 125:0.0223  train auc: 0.993296703297  test auc: 0.999007936508\n",
      " loss at iter 126:0.0222  train auc: 0.993296703297  test auc: 0.999007936508\n",
      " loss at iter 127:0.0221  train auc: 0.993406593407  test auc: 0.999007936508\n",
      " loss at iter 128:0.0220  train auc: 0.993406593407  test auc: 0.999007936508\n",
      " loss at iter 129:0.0218  train auc: 0.993379120879  test auc: 0.999007936508\n",
      " loss at iter 130:0.0217  train auc: 0.993379120879  test auc: 0.999007936508\n",
      " loss at iter 131:0.0215  train auc: 0.993379120879  test auc: 0.999007936508\n",
      " loss at iter 132:0.0214  train auc: 0.993379120879  test auc: 0.999503968254\n",
      " loss at iter 133:0.0211  train auc: 0.993379120879  test auc: 0.999503968254\n",
      " loss at iter 134:0.0209  train auc: 0.993434065934  test auc: 0.999503968254\n",
      " loss at iter 135:0.0206  train auc: 0.993489010989  test auc: 0.999503968254\n",
      " loss at iter 136:0.0203  train auc: 0.993489010989  test auc: 0.999503968254\n",
      " loss at iter 137:0.0199  train auc: 0.993489010989  test auc: 0.999503968254\n",
      " loss at iter 138:0.0194  train auc: 0.993598901099  test auc: 0.999503968254\n",
      " loss at iter 139:0.0189  train auc: 0.993598901099  test auc: 0.999503968254\n",
      " loss at iter 140:0.0184  train auc: 0.993653846154  test auc: 0.999503968254\n",
      " loss at iter 141:0.0179  train auc: 0.993763736264  test auc: 0.999503968254\n",
      " loss at iter 142:0.0173  train auc: 0.993873626374  test auc: 0.999503968254\n",
      " loss at iter 143:0.0168  train auc: 0.993928571429  test auc: 0.999503968254\n",
      " loss at iter 144:0.0163  train auc: 0.993983516484  test auc: 0.999503968254\n",
      " loss at iter 145:0.0159  train auc: 0.993983516484  test auc: 0.999503968254\n",
      " loss at iter 146:0.0154  train auc: 0.994038461538  test auc: 0.999503968254\n",
      " loss at iter 147:0.0150  train auc: 0.994038461538  test auc: 0.999503968254\n",
      " loss at iter 148:0.0146  train auc: 0.994038461538  test auc: 0.999503968254\n",
      " loss at iter 149:0.0143  train auc: 0.994093406593  test auc: 0.999503968254\n",
      " loss at iter 150:0.0140  train auc: 0.994093406593  test auc: 0.999503968254\n",
      " loss at iter 151:0.0137  train auc: 0.994203296703  test auc: 0.999503968254\n",
      " loss at iter 152:0.0135  train auc: 0.994203296703  test auc: 0.999503968254\n",
      " loss at iter 153:0.0133  train auc: 0.994203296703  test auc: 0.999503968254\n",
      " loss at iter 154:0.0131  train auc: 0.994203296703  test auc: 0.999503968254\n",
      " loss at iter 155:0.0129  train auc: 0.994203296703  test auc: 0.999503968254\n",
      " loss at iter 156:0.0128  train auc: 0.994203296703  test auc: 0.999503968254\n",
      " loss at iter 157:0.0126  train auc: 0.994258241758  test auc: 0.999503968254\n",
      " loss at iter 158:0.0125  train auc: 0.994258241758  test auc: 0.999503968254\n",
      " loss at iter 159:0.0124  train auc: 0.994258241758  test auc: 0.999503968254\n",
      " loss at iter 160:0.0123  train auc: 0.994258241758  test auc: 0.999503968254\n",
      " loss at iter 161:0.0122  train auc: 0.994258241758  test auc: 0.999503968254\n",
      " loss at iter 162:0.0122  train auc: 0.994258241758  test auc: 0.999503968254\n",
      " loss at iter 163:0.0121  train auc: 0.994258241758  test auc: 0.999503968254\n",
      " loss at iter 164:0.0120  train auc: 0.994258241758  test auc: 0.999503968254\n",
      " loss at iter 165:0.0120  train auc: 0.994258241758  test auc: 0.999503968254\n",
      " loss at iter 166:0.0119  train auc: 0.994258241758  test auc: 0.999503968254\n",
      " loss at iter 167:0.0119  train auc: 0.994258241758  test auc: 0.999503968254\n",
      " loss at iter 168:0.0119  train auc: 0.994258241758  test auc: 0.999503968254\n",
      " loss at iter 169:0.0118  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 170:0.0118  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 171:0.0117  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 172:0.0117  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 173:0.0117  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 174:0.0117  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 175:0.0116  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 176:0.0116  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 177:0.0116  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 178:0.0116  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 179:0.0116  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 180:0.0116  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 181:0.0115  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 182:0.0115  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 183:0.0115  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 184:0.0115  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 185:0.0115  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 186:0.0115  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 187:0.0115  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 188:0.0115  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 189:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 190:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 191:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 192:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 193:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 194:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 195:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 196:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 197:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 198:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 199:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 200:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 201:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 202:0.0114  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 203:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 204:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 205:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 206:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 207:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 208:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 209:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 210:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 211:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 212:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 213:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 214:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 215:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 216:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 217:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 218:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 219:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 220:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 221:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 222:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 223:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 224:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 225:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 226:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 227:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 228:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 229:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 230:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 231:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 232:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 233:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 234:0.0113  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 235:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 236:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 237:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 238:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 239:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 240:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 241:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 242:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 243:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 244:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 245:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 246:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 247:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 248:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 249:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 250:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 251:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 252:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 253:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 254:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 255:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 256:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 257:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 258:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 259:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 260:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 261:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 262:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 263:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 264:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 265:0.0112  train auc: 0.994313186813  test auc: 0.999503968254\n",
      " loss at iter 266:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 267:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 268:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 269:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 270:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 271:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 272:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 273:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 274:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 275:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 276:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 277:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 278:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 279:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 280:0.0112  train auc: 0.994340659341  test auc: 0.999503968254\n",
      " loss at iter 281:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 282:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 283:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 284:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 285:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 286:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 287:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 288:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 289:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 290:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 291:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 292:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 293:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 294:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 295:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 296:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 297:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 298:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 299:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 300:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 301:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 302:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 303:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 304:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 305:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 306:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 307:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 308:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 309:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 310:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 311:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 312:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 313:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 314:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 315:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 316:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 317:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 318:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 319:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 320:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 321:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 322:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 323:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 324:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 325:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 326:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 327:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 328:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 329:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 330:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 331:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 332:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 333:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 334:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 335:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 336:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 337:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 338:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 339:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 340:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 341:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 342:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 343:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 344:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 345:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 346:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 347:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 348:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 349:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 350:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 351:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 352:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 353:0.0112  train auc: 0.994395604396  test auc: 0.999503968254\n",
      " loss at iter 354:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 355:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 356:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 357:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 358:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 359:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 360:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 361:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 362:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 363:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 364:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 365:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 366:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 367:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 368:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 369:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 370:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 371:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 372:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 373:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 374:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 375:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 376:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 377:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 378:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 379:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 380:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 381:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 382:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 383:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 384:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 385:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 386:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 387:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 388:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 389:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 390:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 391:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 392:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 393:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 394:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 395:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 396:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 397:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 398:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 399:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 400:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 401:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 402:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 403:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 404:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 405:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 406:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 407:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 408:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 409:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 410:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 411:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 412:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 413:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 414:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 415:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 416:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 417:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 418:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 419:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 420:0.0112  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 421:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 422:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 423:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 424:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 425:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 426:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 427:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 428:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 429:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 430:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 431:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 432:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 433:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 434:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 435:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 436:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 437:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 438:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 439:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 440:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 441:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 442:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 443:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 444:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 445:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 446:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 447:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 448:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 449:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 450:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 451:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 452:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 453:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 454:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 455:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 456:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 457:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 458:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 459:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 460:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 461:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 462:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 463:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 464:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 465:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 466:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 467:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 468:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 469:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 470:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 471:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 472:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 473:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 474:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 475:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 476:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 477:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 478:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 479:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 480:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 481:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 482:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 483:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 484:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 485:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 486:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 487:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 488:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 489:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 490:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 491:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 492:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 493:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 494:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 495:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 496:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 497:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 498:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 499:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 500:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 501:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 502:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 503:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 504:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 505:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 506:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 507:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 508:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 509:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 510:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 511:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 512:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 513:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 514:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 515:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 516:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 517:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 518:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 519:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 520:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 521:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 522:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 523:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 524:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 525:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 526:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 527:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 528:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 529:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 530:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 531:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 532:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 533:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 534:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 535:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 536:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 537:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 538:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 539:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 540:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 541:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 542:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 543:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 544:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 545:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 546:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 547:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 548:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 549:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 550:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 551:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 552:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 553:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 554:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 555:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 556:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 557:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 558:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 559:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 560:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 561:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 562:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 563:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 564:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 565:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 566:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 567:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 568:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 569:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 570:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 571:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 572:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 573:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 574:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 575:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 576:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 577:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 578:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 579:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 580:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 581:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 582:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 583:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 584:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 585:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 586:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 587:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 588:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 589:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 590:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 591:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 592:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 593:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 594:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 595:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 596:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 597:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 598:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 599:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 600:0.0111  train auc: 0.994450549451  test auc: 0.999503968254\n",
      " loss at iter 601:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 602:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 603:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 604:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 605:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 606:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 607:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 608:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 609:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 610:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 611:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 612:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 613:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 614:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 615:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 616:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 617:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 618:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 619:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 620:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 621:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 622:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 623:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 624:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 625:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 626:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 627:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 628:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 629:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 630:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 631:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 632:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 633:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 634:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 635:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 636:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 637:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 638:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 639:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 640:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 641:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 642:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 643:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 644:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 645:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 646:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 647:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 648:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 649:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 650:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 651:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 652:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 653:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 654:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 655:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 656:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 657:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 658:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 659:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 660:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 661:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 662:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 663:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 664:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 665:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 666:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 667:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 668:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 669:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 670:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 671:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 672:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 673:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 674:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 675:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 676:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 677:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 678:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 679:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 680:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 681:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 682:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 683:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 684:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 685:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 686:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 687:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 688:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 689:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 690:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 691:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 692:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 693:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 694:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 695:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 696:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 697:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 698:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 699:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 700:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 701:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 702:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 703:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 704:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 705:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 706:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 707:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 708:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 709:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 710:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 711:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 712:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 713:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 714:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 715:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 716:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 717:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 718:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 719:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 720:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 721:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 722:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 723:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 724:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 725:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 726:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 727:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 728:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 729:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 730:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 731:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 732:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 733:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 734:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 735:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 736:0.0111  train auc: 0.994450549451  test auc: 1.0\n",
      " loss at iter 737:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 738:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 739:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 740:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 741:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 742:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 743:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 744:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 745:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 746:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 747:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 748:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 749:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 750:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 751:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 752:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 753:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 754:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 755:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 756:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 757:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 758:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 759:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 760:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 761:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 762:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 763:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 764:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 765:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 766:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 767:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 768:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 769:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 770:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 771:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 772:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 773:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 774:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 775:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 776:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 777:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 778:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 779:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 780:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 781:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 782:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 783:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 784:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 785:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 786:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 787:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 788:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 789:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 790:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 791:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 792:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 793:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 794:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 795:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 796:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 797:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 798:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 799:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 800:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 801:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 802:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 803:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 804:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 805:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 806:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 807:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 808:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 809:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 810:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 811:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 812:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 813:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 814:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 815:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 816:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 817:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 818:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 819:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 820:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 821:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 822:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 823:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 824:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 825:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 826:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 827:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 828:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 829:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 830:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 831:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 832:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 833:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 834:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 835:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 836:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 837:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 838:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 839:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 840:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 841:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 842:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 843:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 844:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 845:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 846:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 847:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 848:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 849:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 850:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 851:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 852:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 853:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 854:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 855:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 856:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 857:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 858:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 859:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 860:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 861:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 862:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 863:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 864:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 865:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 866:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 867:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 868:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 869:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 870:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 871:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 872:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 873:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 874:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 875:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 876:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 877:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 878:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 879:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 880:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 881:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 882:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 883:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 884:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 885:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 886:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 887:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 888:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 889:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 890:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 891:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 892:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 893:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 894:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 895:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 896:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 897:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 898:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 899:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 900:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 901:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 902:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 903:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 904:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 905:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 906:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 907:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 908:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 909:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 910:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 911:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 912:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 913:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 914:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 915:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 916:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 917:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 918:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 919:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 920:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 921:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 922:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 923:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 924:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 925:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 926:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 927:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 928:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 929:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 930:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 931:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 932:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 933:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 934:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 935:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 936:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 937:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 938:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 939:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 940:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 941:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 942:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 943:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 944:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 945:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 946:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 947:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 948:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 949:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 950:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 951:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 952:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 953:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 954:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 955:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 956:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 957:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 958:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 959:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 960:0.0111  train auc: 0.994505494505  test auc: 1.0\n",
      " loss at iter 961:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 962:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 963:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 964:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 965:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 966:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 967:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 968:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 969:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 970:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 971:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 972:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 973:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 974:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 975:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 976:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 977:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 978:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 979:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 980:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 981:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 982:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 983:0.0111  train auc: 0.99456043956  test auc: 1.0\n",
      " loss at iter 984:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 985:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 986:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 987:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 988:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 989:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 990:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 991:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 992:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 993:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 994:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 995:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 996:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 997:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 998:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      " loss at iter 999:0.0111  train auc: 0.994615384615  test auc: 1.0\n",
      "resulting weights:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1161e3050>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASYAAAD8CAYAAADaFgknAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115d091d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for i in range(1000):\n",
    "    loss_i = train_function(X_train,y_train)\n",
    "    print ' loss at iter %i:%.4f' % (i, loss_i),\n",
    "    print ' train auc:', roc_auc_score(y_train, predict_function(X_train)),\n",
    "    print ' test auc:', roc_auc_score(y_test, predict_function(X_test))\n",
    "    \n",
    "print (\"resulting weights:\")\n",
    "plt.imshow(W.get_value().reshape(8,-1))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 align=\"center\">Lasagne</h1>\n",
    "\n",
    "* lasagne - это библиотека для написания нейронок произвольной формы на theano\n",
    "* В качестве демо-задачи выберем то же распознавание чисел, но на большем масштабе задачи, картинки 28x28, 10 цифр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "# print 'X размера', X_train.shape, 'y размера', y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAACqCAYAAAA6El8nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAEAKMAkEAAAAAACQAv8f8nsVTkt/GYcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115e8a090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=7, figsize=(20, 20))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(X_train[i, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Давайте посмотрим на DenseLayer в lasagne\n",
    "- http://lasagne.readthedocs.io/en/latest/modules/layers/dense.html\n",
    "- https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/dense.py#L16-L124 \n",
    "- Весь содаржательный код тут https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/dense.py#L121 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from theano import tensor as T\n",
    "from lasagne.nonlinearities import softmax\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lasagne import init\n",
    "from lasagne.objectives import cross\n",
    "\n",
    "X, y = T.tensor4('X'), T.vector('y', 'int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Так задаётся архитектура нейронки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#входной слой (вспомогательный)\n",
    "net = lasagne.layers.InputLayer(shape=(None, 1, 28, 28), input_var=X)\n",
    "\n",
    "# init.Constant() -- плохо\n",
    "net = lasagne.layers.Conv2DLayer(net, 15, 3, pad='valid') # сверточный слой\n",
    "net = lasagne.layers.Conv2DLayer(net, 10,  2, pad='full')  # сверточный слой\n",
    "\n",
    "net = lasagne.layers.DenseLayer(net, num_units=500) # полносвязный слой\n",
    "net = lasagne.layers.DropoutLayer(net, 0.5)         # регуляризатор\n",
    "net = lasagne.layers.DenseLayer(net, num_units=200) # полносвязный слой\n",
    "\n",
    "# плохо\n",
    "# net = lasagne.layers.DenseLayer(net, num_units=10)  # полносвязный слой\n",
    "net = lasagne.layers.DenseLayer(net, num_units=10, nonlinearity=lasagne.nonlinearities.softmax)  # полносвязный слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#предсказание нейронки (theano-преобразование)\n",
    "y_predicted = lasagne.layers.get_output(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, b, W, b, W, b, W, b, W, b]\n"
     ]
    }
   ],
   "source": [
    "#все веса нейронки (shared-переменные)\n",
    "# all_weights = lasagne.layers.get_all_params(net)\n",
    "all_weights = lasagne.layers.get_all_params(net, trainable=True)\n",
    "print all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#функция ошибки и точности будет прямо внутри\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted, y).mean()\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#сразу посчитать словарь обновлённых значений с шагом по градиенту, как раньше\n",
    "updates = lasagne.updates.momentum(loss, all_weights, learning_rate=0.01, momentum=0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#функция, делает updates и возвращащет значение функции потерь и точности\n",
    "train_fun = theano.function([X, y], [loss, accuracy], updates=updates)\n",
    "accuracy_fun = theano.function([X, y], accuracy) # точность без обновления весов, для теста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Процесс обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10 took\n",
      "\t training loss:\t\t 0.41485\n",
      "\t train accuracy:\t 87.064\n",
      "\t validation accuracy:\t 93.52\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-03a5426a2b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/tensor/blas.pyc\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0;31m# The error raised by numpy has no shape information, we mean to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from mnist import iterate_minibatches\n",
    "\n",
    "num_epochs = 10 #количество проходов по данным\n",
    "batch_size = 100 #размер мини-батча\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err, train_acc, train_batches = 0, 0, 0\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc, val_batches = 0, 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print('Epoch %s of %s took' % (epoch + 1, num_epochs))\n",
    "    print('\\t training loss:\\t\\t %.5f' % (train_err / train_batches))\n",
    "    print('\\t train accuracy:\\t %s' % (train_acc / train_batches * 100))\n",
    "    print('\\t validation accuracy:\\t %s' % (val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results: \\n test accuracy:\\t\\t{:.2f} %\".format(test_acc / test_batches * 100))чы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# не забывайте оставлять отзывы \n",
    "# о лекции https://goo.gl/gMeYNL о семинаре https://goo.gl/5hlPD0 :)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
