{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 align=\"center\">Lecture</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Активации из сетей для классификации, это хорошие признаки для изображений\n",
    "\n",
    "<img src=\"img/act.png\" width=\"800\">\n",
    "\n",
    "\n",
    "## Современные архитектуры очень глубокие , самые модные \n",
    "\n",
    "### VGG (стандартная архитектура, без наворотов)\n",
    "\n",
    "<img src=\"img/vgg.png\" width=\"600\">\n",
    "\n",
    "### ResNet (Shortcut + Batch Normalization)\n",
    " \n",
    "<img src=\"img/resnet.png\" width=\"800\">\n",
    " \n",
    "### GoogleNet (Много раз предсказываем классы на разных уровнях сети)\n",
    "\n",
    " \n",
    "<img src=\"img/gln.png\" width=\"800\">\n",
    "\n",
    "\n",
    "## Чем глубже слой тем более высокоуровневые признаки он детектирует\n",
    "\n",
    "<img src=\"img/feat.png\" width=\"800\">\n",
    "\n",
    "## На практике гораздо проще дообучать уже обученные сети (Fine-Tuning)\n",
    "\n",
    "<img src=\"img/ft.jpg\" width=\"600\">\n",
    "\n",
    "## Dark Magic \n",
    "\n",
    "<img src=\"img/dm.png\" width=\"600\">\n",
    "\n",
    "# Сегодня Theano and Lasagne :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 align=\"center\">Theano</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```bash\n",
    "pip install -U https://github.com/Theano/Theano/archive/master.zip\n",
    "pip install -U https://github.com/Lasagne/Lasagne/archive/master.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Разминка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### будущий параметр функции -- символьная переменная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N = T.scalar('a dimension', dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### рецепт получения квадрата -- орперации над символьными переменным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result = T.power(N, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### theano.grad(cost, wrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grad_result = theano.grad(result, N) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### компиляция функции \"получения квадрата\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sq_function = theano.function(inputs=[N], outputs=result)\n",
    "gr_function = theano.function(inputs=[N], outputs=grad_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### применение функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Заводим np.array x\n",
    "xv = np.arange(-10, 10)\n",
    "\n",
    "# Применяем функцию к каждому x\n",
    "val = map(float, [sq_function(x) for x in xv])\n",
    "\n",
    "# Посичтаем градиент в кажой точке\n",
    "grad = map(float, [gr_function(x) for x in xv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Что мы увидим если нарисуем функцию и градиент?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1053e01d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBAuQhK+EEK4CEn4QgjhIiThCyGEi/h/3cVMHQw86eIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d8566d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.plot(xv, val, label='x*x')\n",
    "pylab.plot(xv, grad, label='d x*x / dx')\n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Как оно работает?\n",
    "* почти всё, что есть в numpy есть в theano tensor и называется так же: `np.mean -> T.mean` и так далее...\n",
    "* `theano.function` умеет за одно обновлять `shared` переменные по рецепту в `updates`\n",
    "* Переменные нужно хранить в `shared` переменных, их можно менять после компиляции `theano.shared(np.ones(10))`\n",
    "\n",
    " \n",
    "Ничего не понятно? Сейчас исправим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Теперь сам, LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X_data, y_data = datasets.load_digits(2, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y метки классов 0 или 1 [форма - (360,)]: [0 1 0 1 0 1 0 0 1 1]\n",
      "X цифорки вытянутые в вектор [форма - (360, 64)]:\n",
      "[[  0.   0.   5.  13.   9.   1.   0.   0.]\n",
      " [  0.   0.  13.  15.  10.  15.   5.   0.]\n",
      " [  0.   3.  15.   2.   0.  11.   8.   0.]\n",
      " [  0.   4.  12.   0.   0.   8.   8.   0.]\n",
      " [  0.   5.   8.   0.   0.   9.   8.   0.]\n",
      " [  0.   4.  11.   0.   1.  12.   7.   0.]\n",
      " [  0.   2.  14.   5.  10.  12.   0.   0.]\n",
      " [  0.   0.   6.  13.  10.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print 'y метки классов 0 или 1 [форма - %s]:' % (str(y_data.shape)),y_data[:10]\n",
    "print 'X цифорки вытянутые в вектор [форма - %s]:' % (str(X_data.shape))\n",
    "print X_data[0].reshape((8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# переменные и входы\n",
    "W = theano.shared(np.random.normal(0, 1, 64))\n",
    "X = T.fmatrix()\n",
    "y = T.fvector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predicted_y = 1 / (1 + T.exp(-T.dot(X, W)))\n",
    "loss = ((y - predicted_y) ** 2).mean()\n",
    "grad = T.grad(loss, W)\n",
    "# {W: <новое значение весов после шага градиентного спуска>}\n",
    "learning_rate = 0.05\n",
    "updates = {W: W - learning_rate * grad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_function = theano.function([X, y], loss, updates=updates, allow_input_downcast=True)\n",
    "predict_function = theano.function([X], predicted_y, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss at iter 0:0.4913  train auc: 0.616526472044  test auc: 0.632653061224\n",
      " loss at iter 1:0.4907  train auc: 0.619962614767  test auc: 0.632653061224\n",
      " loss at iter 2:0.4901  train auc: 0.623453735774  test auc: 0.632653061224\n",
      " loss at iter 3:0.4895  train auc: 0.623453735774  test auc: 0.641114982578\n",
      " loss at iter 4:0.4888  train auc: 0.621529495849  test auc: 0.641612742658\n",
      " loss at iter 5:0.4881  train auc: 0.63153554346  test auc: 0.642110502738\n",
      " loss at iter 6:0.4873  train auc: 0.638462807191  test auc: 0.642110502738\n",
      " loss at iter 7:0.4863  train auc: 0.645555005773  test auc: 0.642608262817\n",
      " loss at iter 8:0.4848  train auc: 0.645829897191  test auc: 0.643106022897\n",
      " loss at iter 9:0.4827  train auc: 0.653087030623  test auc: 0.660029865605\n",
      " loss at iter 10:0.4792  train auc: 0.663697839353  test auc: 0.661025385764\n",
      " loss at iter 11:0.4737  train auc: 0.670954972786  test auc: 0.661523145844\n",
      " loss at iter 12:0.4665  train auc: 0.67194458189  test auc: 0.66600298656\n",
      " loss at iter 13:0.4592  train auc: 0.695640222112  test auc: 0.675709308113\n",
      " loss at iter 14:0.4524  train auc: 0.71020946726  test auc: 0.685415629667\n",
      " loss at iter 15:0.4446  train auc: 0.71834625323  test auc: 0.704828272773\n",
      " loss at iter 16:0.4340  train auc: 0.724283907856  test auc: 0.72175211548\n",
      " loss at iter 17:0.4203  train auc: 0.744021111661  test auc: 0.73568939771\n",
      " loss at iter 18:0.4004  train auc: 0.755566551212  test auc: 0.742409158785\n",
      " loss at iter 19:0.3738  train auc: 0.768568915278  test auc: 0.751119960179\n",
      " loss at iter 20:0.3583  train auc: 0.785529715762  test auc: 0.768541562967\n",
      " loss at iter 21:0.3429  train auc: 0.800428830612  test auc: 0.785465405674\n",
      " loss at iter 22:0.3210  train auc: 0.817389631096  test auc: 0.796665007466\n",
      " loss at iter 23:0.3001  train auc: 0.818571664193  test auc: 0.814086610254\n",
      " loss at iter 24:0.2807  train auc: 0.82912749464  test auc: 0.817570930811\n",
      " loss at iter 25:0.2507  train auc: 0.836714497773  test auc: 0.81956197113\n",
      " loss at iter 26:0.2379  train auc: 0.844081587773  test auc: 0.821553011448\n",
      " loss at iter 27:0.2275  train auc: 0.848232448183  test auc: 0.822548531608\n",
      " loss at iter 28:0.2182  train auc: 0.855379625048  test auc: 0.824539571926\n",
      " loss at iter 29:0.2132  train auc: 0.856121831876  test auc: 0.825037332006\n",
      " loss at iter 30:0.2097  train auc: 0.857166419264  test auc: 0.834245893479\n",
      " loss at iter 31:0.2066  train auc: 0.857826158667  test auc: 0.835739173718\n",
      " loss at iter 32:0.2030  train auc: 0.858458408928  test auc: 0.843952215032\n",
      " loss at iter 33:0.1980  train auc: 0.862554291055  test auc: 0.852663016426\n",
      " loss at iter 34:0.1894  train auc: 0.870251250756  test auc: 0.853160776506\n",
      " loss at iter 35:0.1764  train auc: 0.877618340755  test auc: 0.854156296665\n",
      " loss at iter 36:0.1665  train auc: 0.87937764583  test auc: 0.855649576904\n",
      " loss at iter 37:0.1601  train auc: 0.880752102919  test auc: 0.855649576904\n",
      " loss at iter 38:0.1544  train auc: 0.881851668591  test auc: 0.858636137382\n",
      " loss at iter 39:0.1496  train auc: 0.883006212546  test auc: 0.858636137382\n",
      " loss at iter 40:0.1457  train auc: 0.884160756501  test auc: 0.859631657541\n",
      " loss at iter 41:0.1425  train auc: 0.885535213591  test auc: 0.8606271777\n",
      " loss at iter 42:0.1400  train auc: 0.88690967068  test auc: 0.862120457939\n",
      " loss at iter 43:0.1378  train auc: 0.887569410083  test auc: 0.862618218019\n",
      " loss at iter 44:0.1360  train auc: 0.888504040904  test auc: 0.863115978099\n",
      " loss at iter 45:0.1345  train auc: 0.889493650008  test auc: 0.863613738178\n",
      " loss at iter 46:0.1331  train auc: 0.890098411128  test auc: 0.864111498258\n",
      " loss at iter 47:0.1319  train auc: 0.890758150531  test auc: 0.864609258337\n",
      " loss at iter 48:0.1309  train auc: 0.891802737919  test auc: 0.865107018417\n",
      " loss at iter 49:0.1299  train auc: 0.892517455605  test auc: 0.865107018417\n",
      " loss at iter 50:0.1290  train auc: 0.893012260157  test auc: 0.866600298656\n",
      " loss at iter 51:0.1281  train auc: 0.893507064709  test auc: 0.867595818815\n",
      " loss at iter 52:0.1273  train auc: 0.893891912694  test auc: 0.867595818815\n",
      " loss at iter 53:0.1266  train auc: 0.894386717247  test auc: 0.868591338975\n",
      " loss at iter 54:0.1259  train auc: 0.894716586948  test auc: 0.868591338975\n",
      " loss at iter 55:0.1253  train auc: 0.89504645665  test auc: 0.868591338975\n",
      " loss at iter 56:0.1247  train auc: 0.895486282918  test auc: 0.868591338975\n",
      " loss at iter 57:0.1241  train auc: 0.895926109187  test auc: 0.868591338975\n",
      " loss at iter 58:0.1235  train auc: 0.896530870306  test auc: 0.868591338975\n",
      " loss at iter 59:0.1228  train auc: 0.897080653142  test auc: 0.870084619214\n",
      " loss at iter 60:0.1221  train auc: 0.897465501127  test auc: 0.871080139373\n",
      " loss at iter 61:0.1213  train auc: 0.897960305679  test auc: 0.871080139373\n",
      " loss at iter 62:0.1203  train auc: 0.898730001649  test auc: 0.871577899452\n",
      " loss at iter 63:0.1189  train auc: 0.899169827918  test auc: 0.872075659532\n",
      " loss at iter 64:0.1170  train auc: 0.900049480455  test auc: 0.872573419612\n",
      " loss at iter 65:0.1146  train auc: 0.900709219858  test auc: 0.872573419612\n",
      " loss at iter 66:0.1120  train auc: 0.901533894112  test auc: 0.87456445993\n",
      " loss at iter 67:0.1097  train auc: 0.902083676948  test auc: 0.876555500249\n",
      " loss at iter 68:0.1080  train auc: 0.902853372918  test auc: 0.877053260329\n",
      " loss at iter 69:0.1067  train auc: 0.90334817747  test auc: 0.877551020408\n",
      " loss at iter 70:0.1056  train auc: 0.903897960306  test auc: 0.879044300647\n",
      " loss at iter 71:0.1046  train auc: 0.904337786574  test auc: 0.886510701842\n",
      " loss at iter 72:0.1037  train auc: 0.904777612843  test auc: 0.887506222001\n",
      " loss at iter 73:0.1028  train auc: 0.905327395679  test auc: 0.887506222001\n",
      " loss at iter 74:0.1018  train auc: 0.905767221947  test auc: 0.887506222001\n",
      " loss at iter 75:0.1009  train auc: 0.906317004783  test auc: 0.887506222001\n",
      " loss at iter 76:0.0998  train auc: 0.906756831052  test auc: 0.887506222001\n",
      " loss at iter 77:0.0986  train auc: 0.907581505305  test auc: 0.88850174216\n",
      " loss at iter 78:0.0974  train auc: 0.908186266425  test auc: 0.88850174216\n",
      " loss at iter 79:0.0961  train auc: 0.908736049261  test auc: 0.88850174216\n",
      " loss at iter 80:0.0948  train auc: 0.909285832096  test auc: 0.88850174216\n",
      " loss at iter 81:0.0934  train auc: 0.910220462917  test auc: 0.88949726232\n",
      " loss at iter 82:0.0922  train auc: 0.911045137171  test auc: 0.890990542558\n",
      " loss at iter 83:0.0910  train auc: 0.91426136676  test auc: 0.891488302638\n",
      " loss at iter 84:0.0899  train auc: 0.914976084447  test auc: 0.891488302638\n",
      " loss at iter 85:0.0890  train auc: 0.915195997581  test auc: 0.891986062718\n",
      " loss at iter 86:0.0882  train auc: 0.915470888999  test auc: 0.891986062718\n",
      " loss at iter 87:0.0876  train auc: 0.915855736984  test auc: 0.891986062718\n",
      " loss at iter 88:0.0870  train auc: 0.916130628402  test auc: 0.892981582877\n",
      " loss at iter 89:0.0866  train auc: 0.91657045467  test auc: 0.893479342957\n",
      " loss at iter 90:0.0861  train auc: 0.916845346088  test auc: 0.893728222997\n",
      " loss at iter 91:0.0858  train auc: 0.91717521579  test auc: 0.894225983076\n",
      " loss at iter 92:0.0855  train auc: 0.917395128924  test auc: 0.894225983076\n",
      " loss at iter 93:0.0852  train auc: 0.917724998626  test auc: 0.894225983076\n",
      " loss at iter 94:0.0849  train auc: 0.917999890043  test auc: 0.895719263315\n",
      " loss at iter 95:0.0847  train auc: 0.918329759745  test auc: 0.895719263315\n",
      " loss at iter 96:0.0844  train auc: 0.918687118588  test auc: 0.896217023395\n",
      " loss at iter 97:0.0842  train auc: 0.91901698829  test auc: 0.896217023395\n",
      " loss at iter 98:0.0840  train auc: 0.919346857991  test auc: 0.896217023395\n",
      " loss at iter 99:0.0838  train auc: 0.919511792842  test auc: 0.896217023395\n",
      " loss at iter 100:0.0836  train auc: 0.919676727693  test auc: 0.896217023395\n",
      " loss at iter 101:0.0834  train auc: 0.920006597394  test auc: 0.896217023395\n",
      " loss at iter 102:0.0832  train auc: 0.920116553961  test auc: 0.896714783474\n",
      " loss at iter 103:0.0830  train auc: 0.920501401946  test auc: 0.896714783474\n",
      " loss at iter 104:0.0828  train auc: 0.920913739073  test auc: 0.896714783474\n",
      " loss at iter 105:0.0826  train auc: 0.920913739073  test auc: 0.896714783474\n",
      " loss at iter 106:0.0823  train auc: 0.921243608775  test auc: 0.896714783474\n",
      " loss at iter 107:0.0821  train auc: 0.921573478476  test auc: 0.896714783474\n",
      " loss at iter 108:0.0818  train auc: 0.921765902469  test auc: 0.897212543554\n",
      " loss at iter 109:0.0816  train auc: 0.921930837319  test auc: 0.897212543554\n",
      " loss at iter 110:0.0813  train auc: 0.922040793886  test auc: 0.897212543554\n",
      " loss at iter 111:0.0811  train auc: 0.922260707021  test auc: 0.897212543554\n",
      " loss at iter 112:0.0808  train auc: 0.922590576722  test auc: 0.897212543554\n",
      " loss at iter 113:0.0805  train auc: 0.922728022431  test auc: 0.898705823793\n",
      " loss at iter 114:0.0803  train auc: 0.922920446424  test auc: 0.898705823793\n",
      " loss at iter 115:0.0800  train auc: 0.923140359558  test auc: 0.899701343952\n",
      " loss at iter 116:0.0797  train auc: 0.923250316125  test auc: 0.899701343952\n",
      " loss at iter 117:0.0794  train auc: 0.923415250976  test auc: 0.899701343952\n",
      " loss at iter 118:0.0791  train auc: 0.923470229259  test auc: 0.899701343952\n",
      " loss at iter 119:0.0789  train auc: 0.923772609819  test auc: 0.900199104032\n",
      " loss at iter 120:0.0786  train auc: 0.92393754467  test auc: 0.900199104032\n",
      " loss at iter 121:0.0783  train auc: 0.924157457804  test auc: 0.900199104032\n",
      " loss at iter 122:0.0780  train auc: 0.924322392655  test auc: 0.900199104032\n",
      " loss at iter 123:0.0777  train auc: 0.924432349222  test auc: 0.900696864111\n",
      " loss at iter 124:0.0775  train auc: 0.924542305789  test auc: 0.900696864111\n",
      " loss at iter 125:0.0772  train auc: 0.924817197207  test auc: 0.900696864111\n",
      " loss at iter 126:0.0770  train auc: 0.924872175491  test auc: 0.901194624191\n",
      " loss at iter 127:0.0768  train auc: 0.924927153774  test auc: 0.901194624191\n",
      " loss at iter 128:0.0766  train auc: 0.925037110341  test auc: 0.901194624191\n",
      " loss at iter 129:0.0764  train auc: 0.925257023476  test auc: 0.901692384271\n",
      " loss at iter 130:0.0762  train auc: 0.925312001759  test auc: 0.90219014435\n",
      " loss at iter 131:0.0761  train auc: 0.925531914894  test auc: 0.90219014435\n",
      " loss at iter 132:0.0759  train auc: 0.925641871461  test auc: 0.90268790443\n",
      " loss at iter 133:0.0758  train auc: 0.925861784595  test auc: 0.90268790443\n",
      " loss at iter 134:0.0757  train auc: 0.926191654297  test auc: 0.90268790443\n",
      " loss at iter 135:0.0756  train auc: 0.92624663258  test auc: 0.90268790443\n",
      " loss at iter 136:0.0754  train auc: 0.926356589147  test auc: 0.90268790443\n",
      " loss at iter 137:0.0753  train auc: 0.926466545714  test auc: 0.910403185665\n",
      " loss at iter 138:0.0753  train auc: 0.926631480565  test auc: 0.910403185665\n",
      " loss at iter 139:0.0752  train auc: 0.926796415416  test auc: 0.910403185665\n",
      " loss at iter 140:0.0751  train auc: 0.926851393699  test auc: 0.910900945744\n",
      " loss at iter 141:0.0750  train auc: 0.926906371983  test auc: 0.910900945744\n",
      " loss at iter 142:0.0750  train auc: 0.927071306834  test auc: 0.910900945744\n",
      " loss at iter 143:0.0749  train auc: 0.927181263401  test auc: 0.910900945744\n",
      " loss at iter 144:0.0748  train auc: 0.927236241685  test auc: 0.910900945744\n",
      " loss at iter 145:0.0748  train auc: 0.927291219968  test auc: 0.911398705824\n",
      " loss at iter 146:0.0748  train auc: 0.927346198252  test auc: 0.911398705824\n",
      " loss at iter 147:0.0747  train auc: 0.927456154819  test auc: 0.911398705824\n",
      " loss at iter 148:0.0747  train auc: 0.927511133102  test auc: 0.911398705824\n",
      " loss at iter 149:0.0746  train auc: 0.927676067953  test auc: 0.911896465903\n",
      " loss at iter 150:0.0746  train auc: 0.927676067953  test auc: 0.911896465903\n",
      " loss at iter 151:0.0746  train auc: 0.927676067953  test auc: 0.911896465903\n",
      " loss at iter 152:0.0746  train auc: 0.927841002804  test auc: 0.911896465903\n",
      " loss at iter 153:0.0745  train auc: 0.927841002804  test auc: 0.911896465903\n",
      " loss at iter 154:0.0745  train auc: 0.927895981087  test auc: 0.911896465903\n",
      " loss at iter 155:0.0745  train auc: 0.927895981087  test auc: 0.911896465903\n",
      " loss at iter 156:0.0745  train auc: 0.928115894222  test auc: 0.911896465903\n",
      " loss at iter 157:0.0744  train auc: 0.928115894222  test auc: 0.911896465903\n",
      " loss at iter 158:0.0744  train auc: 0.928115894222  test auc: 0.911896465903\n",
      " loss at iter 159:0.0744  train auc: 0.928115894222  test auc: 0.911896465903\n",
      " loss at iter 160:0.0744  train auc: 0.928170872505  test auc: 0.912394225983\n",
      " loss at iter 161:0.0744  train auc: 0.928225850789  test auc: 0.912394225983\n",
      " loss at iter 162:0.0744  train auc: 0.928225850789  test auc: 0.912394225983\n",
      " loss at iter 163:0.0743  train auc: 0.928225850789  test auc: 0.912891986063\n",
      " loss at iter 164:0.0743  train auc: 0.928225850789  test auc: 0.912891986063\n",
      " loss at iter 165:0.0743  train auc: 0.928225850789  test auc: 0.912891986063\n",
      " loss at iter 166:0.0743  train auc: 0.928280829073  test auc: 0.913389746142\n",
      " loss at iter 167:0.0743  train auc: 0.928445763923  test auc: 0.913389746142\n",
      " loss at iter 168:0.0743  train auc: 0.928445763923  test auc: 0.913389746142\n",
      " loss at iter 169:0.0743  train auc: 0.928445763923  test auc: 0.913389746142\n",
      " loss at iter 170:0.0743  train auc: 0.928445763923  test auc: 0.913887506222\n",
      " loss at iter 171:0.0742  train auc: 0.928445763923  test auc: 0.913887506222\n",
      " loss at iter 172:0.0742  train auc: 0.928445763923  test auc: 0.913887506222\n",
      " loss at iter 173:0.0742  train auc: 0.928445763923  test auc: 0.913887506222\n",
      " loss at iter 174:0.0742  train auc: 0.92855572049  test auc: 0.913887506222\n",
      " loss at iter 175:0.0742  train auc: 0.928610698774  test auc: 0.913887506222\n",
      " loss at iter 176:0.0742  train auc: 0.928720655341  test auc: 0.913887506222\n",
      " loss at iter 177:0.0742  train auc: 0.928720655341  test auc: 0.913887506222\n",
      " loss at iter 178:0.0742  train auc: 0.928775633625  test auc: 0.914385266302\n",
      " loss at iter 179:0.0742  train auc: 0.928775633625  test auc: 0.914385266302\n",
      " loss at iter 180:0.0742  train auc: 0.928775633625  test auc: 0.914385266302\n",
      " loss at iter 181:0.0742  train auc: 0.928830611908  test auc: 0.914385266302\n",
      " loss at iter 182:0.0742  train auc: 0.928830611908  test auc: 0.914385266302\n",
      " loss at iter 183:0.0742  train auc: 0.928830611908  test auc: 0.914385266302\n",
      " loss at iter 184:0.0742  train auc: 0.928830611908  test auc: 0.914385266302\n",
      " loss at iter 185:0.0741  train auc: 0.928830611908  test auc: 0.914385266302\n",
      " loss at iter 186:0.0741  train auc: 0.928940568475  test auc: 0.914385266302\n",
      " loss at iter 187:0.0741  train auc: 0.929050525043  test auc: 0.914385266302\n",
      " loss at iter 188:0.0741  train auc: 0.929050525043  test auc: 0.914385266302\n",
      " loss at iter 189:0.0741  train auc: 0.929050525043  test auc: 0.914385266302\n",
      " loss at iter 190:0.0741  train auc: 0.929050525043  test auc: 0.914385266302\n",
      " loss at iter 191:0.0741  train auc: 0.929105503326  test auc: 0.914385266302\n",
      " loss at iter 192:0.0741  train auc: 0.929105503326  test auc: 0.914385266302\n",
      " loss at iter 193:0.0741  train auc: 0.929105503326  test auc: 0.914385266302\n",
      " loss at iter 194:0.0741  train auc: 0.929105503326  test auc: 0.914385266302\n",
      " loss at iter 195:0.0741  train auc: 0.929215459893  test auc: 0.914385266302\n",
      " loss at iter 196:0.0741  train auc: 0.929215459893  test auc: 0.914385266302\n",
      " loss at iter 197:0.0741  train auc: 0.932321732915  test auc: 0.914385266302\n",
      " loss at iter 198:0.0741  train auc: 0.932321732915  test auc: 0.914385266302\n",
      " loss at iter 199:0.0741  train auc: 0.932486667766  test auc: 0.914385266302\n",
      " loss at iter 200:0.0741  train auc: 0.93254164605  test auc: 0.914385266302\n",
      " loss at iter 201:0.0741  train auc: 0.93254164605  test auc: 0.914385266302\n",
      " loss at iter 202:0.0741  train auc: 0.932596624333  test auc: 0.914385266302\n",
      " loss at iter 203:0.0740  train auc: 0.932651602617  test auc: 0.914385266302\n",
      " loss at iter 204:0.0740  train auc: 0.932706580901  test auc: 0.914385266302\n",
      " loss at iter 205:0.0740  train auc: 0.932706580901  test auc: 0.914385266302\n",
      " loss at iter 206:0.0740  train auc: 0.932706580901  test auc: 0.914385266302\n",
      " loss at iter 207:0.0740  train auc: 0.932706580901  test auc: 0.914385266302\n",
      " loss at iter 208:0.0740  train auc: 0.932706580901  test auc: 0.914385266302\n",
      " loss at iter 209:0.0740  train auc: 0.932706580901  test auc: 0.914385266302\n",
      " loss at iter 210:0.0740  train auc: 0.932816537468  test auc: 0.914385266302\n",
      " loss at iter 211:0.0740  train auc: 0.932816537468  test auc: 0.914385266302\n",
      " loss at iter 212:0.0740  train auc: 0.932816537468  test auc: 0.914385266302\n",
      " loss at iter 213:0.0740  train auc: 0.932816537468  test auc: 0.914385266302\n",
      " loss at iter 214:0.0740  train auc: 0.932871515751  test auc: 0.914385266302\n",
      " loss at iter 215:0.0740  train auc: 0.932926494035  test auc: 0.914385266302\n",
      " loss at iter 216:0.0740  train auc: 0.932926494035  test auc: 0.914385266302\n",
      " loss at iter 217:0.0740  train auc: 0.932926494035  test auc: 0.914385266302\n",
      " loss at iter 218:0.0740  train auc: 0.932981472318  test auc: 0.914385266302\n",
      " loss at iter 219:0.0740  train auc: 0.933036450602  test auc: 0.914883026381\n",
      " loss at iter 220:0.0740  train auc: 0.933036450602  test auc: 0.914883026381\n",
      " loss at iter 221:0.0739  train auc: 0.933091428886  test auc: 0.914883026381\n",
      " loss at iter 222:0.0739  train auc: 0.933146407169  test auc: 0.914883026381\n",
      " loss at iter 223:0.0739  train auc: 0.933146407169  test auc: 0.914883026381\n",
      " loss at iter 224:0.0739  train auc: 0.933201385453  test auc: 0.914883026381\n",
      " loss at iter 225:0.0739  train auc: 0.933201385453  test auc: 0.914883026381\n",
      " loss at iter 226:0.0739  train auc: 0.933201385453  test auc: 0.915380786461\n",
      " loss at iter 227:0.0739  train auc: 0.933201385453  test auc: 0.915380786461\n",
      " loss at iter 228:0.0739  train auc: 0.933256363736  test auc: 0.915380786461\n",
      " loss at iter 229:0.0739  train auc: 0.93331134202  test auc: 0.915380786461\n",
      " loss at iter 230:0.0739  train auc: 0.93331134202  test auc: 0.915380786461\n",
      " loss at iter 231:0.0739  train auc: 0.933366320303  test auc: 0.915380786461\n",
      " loss at iter 232:0.0738  train auc: 0.933421298587  test auc: 0.915878546541\n",
      " loss at iter 233:0.0738  train auc: 0.933421298587  test auc: 0.915878546541\n",
      " loss at iter 234:0.0738  train auc: 0.933421298587  test auc: 0.915878546541\n",
      " loss at iter 235:0.0738  train auc: 0.933421298587  test auc: 0.915878546541\n",
      " loss at iter 236:0.0738  train auc: 0.933421298587  test auc: 0.915878546541\n",
      " loss at iter 237:0.0738  train auc: 0.933421298587  test auc: 0.915878546541\n",
      " loss at iter 238:0.0738  train auc: 0.933421298587  test auc: 0.915878546541\n",
      " loss at iter 239:0.0737  train auc: 0.933531255154  test auc: 0.915878546541\n",
      " loss at iter 240:0.0737  train auc: 0.933586233438  test auc: 0.91637630662\n",
      " loss at iter 241:0.0737  train auc: 0.933586233438  test auc: 0.91637630662\n",
      " loss at iter 242:0.0737  train auc: 0.933586233438  test auc: 0.91637630662\n",
      " loss at iter 243:0.0737  train auc: 0.933641211721  test auc: 0.91637630662\n",
      " loss at iter 244:0.0736  train auc: 0.933696190005  test auc: 0.91637630662\n",
      " loss at iter 245:0.0736  train auc: 0.933806146572  test auc: 0.91637630662\n",
      " loss at iter 246:0.0736  train auc: 0.933806146572  test auc: 0.91637630662\n",
      " loss at iter 247:0.0735  train auc: 0.933861124856  test auc: 0.91637630662\n",
      " loss at iter 248:0.0735  train auc: 0.933916103139  test auc: 0.9168740667\n",
      " loss at iter 249:0.0734  train auc: 0.933971081423  test auc: 0.9168740667\n",
      " loss at iter 250:0.0734  train auc: 0.93408103799  test auc: 0.917371826779\n",
      " loss at iter 251:0.0733  train auc: 0.93100225411  test auc: 0.917371826779\n",
      " loss at iter 252:0.0732  train auc: 0.93100225411  test auc: 0.917371826779\n",
      " loss at iter 253:0.0731  train auc: 0.931112210677  test auc: 0.917371826779\n",
      " loss at iter 254:0.0730  train auc: 0.931222167244  test auc: 0.917371826779\n",
      " loss at iter 255:0.0729  train auc: 0.931277145528  test auc: 0.917371826779\n",
      " loss at iter 256:0.0728  train auc: 0.931277145528  test auc: 0.917371826779\n",
      " loss at iter 257:0.0726  train auc: 0.931387102095  test auc: 0.917371826779\n",
      " loss at iter 258:0.0725  train auc: 0.931497058662  test auc: 0.917371826779\n",
      " loss at iter 259:0.0723  train auc: 0.931497058662  test auc: 0.917371826779\n",
      " loss at iter 260:0.0721  train auc: 0.931661993513  test auc: 0.917371826779\n",
      " loss at iter 261:0.0718  train auc: 0.93177195008  test auc: 0.917869586859\n",
      " loss at iter 262:0.0716  train auc: 0.931909395789  test auc: 0.918367346939\n",
      " loss at iter 263:0.0713  train auc: 0.932129308923  test auc: 0.918367346939\n",
      " loss at iter 264:0.0710  train auc: 0.932514156908  test auc: 0.918367346939\n",
      " loss at iter 265:0.0707  train auc: 0.932624113475  test auc: 0.918367346939\n",
      " loss at iter 266:0.0705  train auc: 0.932734070042  test auc: 0.918367346939\n",
      " loss at iter 267:0.0702  train auc: 0.932899004893  test auc: 0.918367346939\n",
      " loss at iter 268:0.0699  train auc: 0.93300896146  test auc: 0.918367346939\n",
      " loss at iter 269:0.0696  train auc: 0.933393809445  test auc: 0.918367346939\n",
      " loss at iter 270:0.0692  train auc: 0.933723679147  test auc: 0.918367346939\n",
      " loss at iter 271:0.0689  train auc: 0.933998570565  test auc: 0.918367346939\n",
      " loss at iter 272:0.0685  train auc: 0.934218483699  test auc: 0.918367346939\n",
      " loss at iter 273:0.0681  train auc: 0.934328440266  test auc: 0.918367346939\n",
      " loss at iter 274:0.0677  train auc: 0.934438396833  test auc: 0.918367346939\n",
      " loss at iter 275:0.0673  train auc: 0.934603331684  test auc: 0.918865107018\n",
      " loss at iter 276:0.0669  train auc: 0.934768266535  test auc: 0.919362867098\n",
      " loss at iter 277:0.0664  train auc: 0.934823244818  test auc: 0.919860627178\n",
      " loss at iter 278:0.0660  train auc: 0.935180603662  test auc: 0.920856147337\n",
      " loss at iter 279:0.0655  train auc: 0.935428005938  test auc: 0.921851667496\n",
      " loss at iter 280:0.0649  train auc: 0.935702897356  test auc: 0.922349427576\n",
      " loss at iter 281:0.0643  train auc: 0.936142723624  test auc: 0.922349427576\n",
      " loss at iter 282:0.0637  train auc: 0.936582549893  test auc: 0.922847187656\n",
      " loss at iter 283:0.0631  train auc: 0.936802463027  test auc: 0.924340467894\n",
      " loss at iter 284:0.0625  train auc: 0.936912419594  test auc: 0.924340467894\n",
      " loss at iter 285:0.0620  train auc: 0.937077354445  test auc: 0.924340467894\n",
      " loss at iter 286:0.0615  train auc: 0.937517180714  test auc: 0.925335988054\n",
      " loss at iter 287:0.0611  train auc: 0.937792072132  test auc: 0.925335988054\n",
      " loss at iter 288:0.0608  train auc: 0.941228214855  test auc: 0.925335988054\n",
      " loss at iter 289:0.0605  train auc: 0.941558084557  test auc: 0.925335988054\n",
      " loss at iter 290:0.0602  train auc: 0.94161306284  test auc: 0.926331508213\n",
      " loss at iter 291:0.0600  train auc: 0.941723019407  test auc: 0.926829268293\n",
      " loss at iter 292:0.0598  train auc: 0.942052889109  test auc: 0.926829268293\n",
      " loss at iter 293:0.0596  train auc: 0.942272802243  test auc: 0.926829268293\n",
      " loss at iter 294:0.0594  train auc: 0.942547693661  test auc: 0.926829268293\n",
      " loss at iter 295:0.0592  train auc: 0.942712628512  test auc: 0.927327028372\n",
      " loss at iter 296:0.0589  train auc: 0.942767606795  test auc: 0.927327028372\n",
      " loss at iter 297:0.0587  train auc: 0.94298751993  test auc: 0.928322548532\n",
      " loss at iter 298:0.0585  train auc: 0.94315245478  test auc: 0.928322548532\n",
      " loss at iter 299:0.0583  train auc: 0.943482324482  test auc: 0.928322548532\n",
      " loss at iter 300:0.0580  train auc: 0.943647259333  test auc: 0.928322548532\n",
      " loss at iter 301:0.0577  train auc: 0.943702237616  test auc: 0.928322548532\n",
      " loss at iter 302:0.0574  train auc: 0.94392215075  test auc: 0.928322548532\n",
      " loss at iter 303:0.0571  train auc: 0.944032107318  test auc: 0.928820308611\n",
      " loss at iter 304:0.0568  train auc: 0.944306998735  test auc: 0.928820308611\n",
      " loss at iter 305:0.0565  train auc: 0.944471933586  test auc: 0.929318068691\n",
      " loss at iter 306:0.0562  train auc: 0.944636868437  test auc: 0.93031358885\n",
      " loss at iter 307:0.0559  train auc: 0.944801803288  test auc: 0.931309109009\n",
      " loss at iter 308:0.0556  train auc: 0.945021716422  test auc: 0.931309109009\n",
      " loss at iter 309:0.0554  train auc: 0.945241629556  test auc: 0.931309109009\n",
      " loss at iter 310:0.0551  train auc: 0.945571499258  test auc: 0.932304629169\n",
      " loss at iter 311:0.0548  train auc: 0.945736434109  test auc: 0.932802389248\n",
      " loss at iter 312:0.0546  train auc: 0.945846390676  test auc: 0.932802389248\n",
      " loss at iter 313:0.0544  train auc: 0.946176260377  test auc: 0.933300149328\n",
      " loss at iter 314:0.0541  train auc: 0.946231238661  test auc: 0.933300149328\n",
      " loss at iter 315:0.0539  train auc: 0.946341195228  test auc: 0.933300149328\n",
      " loss at iter 316:0.0538  train auc: 0.946561108362  test auc: 0.933797909408\n",
      " loss at iter 317:0.0536  train auc: 0.946781021497  test auc: 0.933797909408\n",
      " loss at iter 318:0.0535  train auc: 0.946781021497  test auc: 0.933797909408\n",
      " loss at iter 319:0.0533  train auc: 0.946890978064  test auc: 0.933797909408\n",
      " loss at iter 320:0.0532  train auc: 0.947000934631  test auc: 0.933797909408\n",
      " loss at iter 321:0.0531  train auc: 0.947055912914  test auc: 0.934295669487\n",
      " loss at iter 322:0.0530  train auc: 0.947165869482  test auc: 0.934295669487\n",
      " loss at iter 323:0.0530  train auc: 0.947330804332  test auc: 0.934793429567\n",
      " loss at iter 324:0.0529  train auc: 0.947385782616  test auc: 0.935788949726\n",
      " loss at iter 325:0.0528  train auc: 0.947385782616  test auc: 0.935788949726\n",
      " loss at iter 326:0.0528  train auc: 0.947495739183  test auc: 0.935788949726\n",
      " loss at iter 327:0.0527  train auc: 0.947550717467  test auc: 0.935788949726\n",
      " loss at iter 328:0.0527  train auc: 0.947660674034  test auc: 0.935788949726\n",
      " loss at iter 329:0.0526  train auc: 0.947825608884  test auc: 0.935788949726\n",
      " loss at iter 330:0.0526  train auc: 0.947990543735  test auc: 0.935788949726\n",
      " loss at iter 331:0.0525  train auc: 0.947990543735  test auc: 0.935788949726\n",
      " loss at iter 332:0.0525  train auc: 0.947990543735  test auc: 0.935788949726\n",
      " loss at iter 333:0.0525  train auc: 0.948045522019  test auc: 0.935788949726\n",
      " loss at iter 334:0.0524  train auc: 0.948155478586  test auc: 0.935788949726\n",
      " loss at iter 335:0.0524  train auc: 0.94821045687  test auc: 0.935788949726\n",
      " loss at iter 336:0.0524  train auc: 0.94821045687  test auc: 0.935788949726\n",
      " loss at iter 337:0.0524  train auc: 0.948265435153  test auc: 0.935788949726\n",
      " loss at iter 338:0.0524  train auc: 0.948265435153  test auc: 0.935788949726\n",
      " loss at iter 339:0.0523  train auc: 0.948265435153  test auc: 0.935788949726\n",
      " loss at iter 340:0.0523  train auc: 0.948320413437  test auc: 0.935788949726\n",
      " loss at iter 341:0.0523  train auc: 0.948320413437  test auc: 0.935788949726\n",
      " loss at iter 342:0.0523  train auc: 0.94837539172  test auc: 0.935788949726\n",
      " loss at iter 343:0.0523  train auc: 0.94837539172  test auc: 0.936286709806\n",
      " loss at iter 344:0.0523  train auc: 0.948540326571  test auc: 0.936286709806\n",
      " loss at iter 345:0.0522  train auc: 0.948540326571  test auc: 0.936286709806\n",
      " loss at iter 346:0.0522  train auc: 0.948650283138  test auc: 0.936286709806\n",
      " loss at iter 347:0.0522  train auc: 0.948705261422  test auc: 0.936286709806\n",
      " loss at iter 348:0.0522  train auc: 0.948705261422  test auc: 0.936286709806\n",
      " loss at iter 349:0.0522  train auc: 0.948705261422  test auc: 0.936286709806\n",
      " loss at iter 350:0.0522  train auc: 0.948705261422  test auc: 0.936286709806\n",
      " loss at iter 351:0.0522  train auc: 0.948815217989  test auc: 0.936286709806\n",
      " loss at iter 352:0.0522  train auc: 0.948815217989  test auc: 0.936286709806\n",
      " loss at iter 353:0.0522  train auc: 0.948815217989  test auc: 0.936286709806\n",
      " loss at iter 354:0.0522  train auc: 0.948815217989  test auc: 0.936286709806\n",
      " loss at iter 355:0.0521  train auc: 0.948815217989  test auc: 0.936286709806\n",
      " loss at iter 356:0.0521  train auc: 0.948815217989  test auc: 0.936286709806\n",
      " loss at iter 357:0.0521  train auc: 0.948815217989  test auc: 0.937282229965\n",
      " loss at iter 358:0.0521  train auc: 0.948815217989  test auc: 0.937779990045\n",
      " loss at iter 359:0.0521  train auc: 0.948815217989  test auc: 0.937779990045\n",
      " loss at iter 360:0.0521  train auc: 0.948815217989  test auc: 0.937779990045\n",
      " loss at iter 361:0.0521  train auc: 0.948815217989  test auc: 0.937779990045\n",
      " loss at iter 362:0.0521  train auc: 0.948815217989  test auc: 0.937779990045\n",
      " loss at iter 363:0.0521  train auc: 0.94898015284  test auc: 0.937779990045\n",
      " loss at iter 364:0.0521  train auc: 0.94898015284  test auc: 0.937779990045\n",
      " loss at iter 365:0.0521  train auc: 0.94898015284  test auc: 0.937779990045\n",
      " loss at iter 366:0.0521  train auc: 0.94898015284  test auc: 0.937779990045\n",
      " loss at iter 367:0.0521  train auc: 0.94898015284  test auc: 0.937779990045\n",
      " loss at iter 368:0.0521  train auc: 0.949035131123  test auc: 0.937779990045\n",
      " loss at iter 369:0.0521  train auc: 0.949035131123  test auc: 0.937779990045\n",
      " loss at iter 370:0.0521  train auc: 0.949035131123  test auc: 0.937779990045\n",
      " loss at iter 371:0.0521  train auc: 0.949035131123  test auc: 0.937779990045\n",
      " loss at iter 372:0.0521  train auc: 0.949035131123  test auc: 0.937779990045\n",
      " loss at iter 373:0.0520  train auc: 0.949035131123  test auc: 0.937779990045\n",
      " loss at iter 374:0.0520  train auc: 0.949035131123  test auc: 0.937779990045\n",
      " loss at iter 375:0.0520  train auc: 0.949035131123  test auc: 0.937779990045\n",
      " loss at iter 376:0.0520  train auc: 0.949035131123  test auc: 0.937779990045\n",
      " loss at iter 377:0.0520  train auc: 0.949035131123  test auc: 0.937779990045\n",
      " loss at iter 378:0.0520  train auc: 0.949035131123  test auc: 0.938277750124\n",
      " loss at iter 379:0.0520  train auc: 0.949035131123  test auc: 0.938277750124\n",
      " loss at iter 380:0.0520  train auc: 0.949035131123  test auc: 0.938277750124\n",
      " loss at iter 381:0.0520  train auc: 0.949035131123  test auc: 0.938277750124\n",
      " loss at iter 382:0.0520  train auc: 0.94914508769  test auc: 0.938277750124\n",
      " loss at iter 383:0.0520  train auc: 0.94914508769  test auc: 0.938277750124\n",
      " loss at iter 384:0.0520  train auc: 0.94914508769  test auc: 0.938277750124\n",
      " loss at iter 385:0.0520  train auc: 0.94914508769  test auc: 0.938277750124\n",
      " loss at iter 386:0.0520  train auc: 0.94914508769  test auc: 0.938277750124\n",
      " loss at iter 387:0.0520  train auc: 0.94914508769  test auc: 0.938277750124\n",
      " loss at iter 388:0.0520  train auc: 0.94914508769  test auc: 0.938277750124\n",
      " loss at iter 389:0.0520  train auc: 0.94914508769  test auc: 0.938277750124\n",
      " loss at iter 390:0.0520  train auc: 0.949255044258  test auc: 0.938277750124\n",
      " loss at iter 391:0.0520  train auc: 0.949255044258  test auc: 0.938277750124\n",
      " loss at iter 392:0.0520  train auc: 0.949255044258  test auc: 0.938277750124\n",
      " loss at iter 393:0.0520  train auc: 0.949255044258  test auc: 0.938277750124\n",
      " loss at iter 394:0.0520  train auc: 0.949255044258  test auc: 0.938277750124\n",
      " loss at iter 395:0.0520  train auc: 0.949310022541  test auc: 0.938277750124\n",
      " loss at iter 396:0.0520  train auc: 0.949310022541  test auc: 0.938277750124\n",
      " loss at iter 397:0.0520  train auc: 0.949365000825  test auc: 0.938277750124\n",
      " loss at iter 398:0.0520  train auc: 0.949365000825  test auc: 0.938277750124\n",
      " loss at iter 399:0.0520  train auc: 0.949365000825  test auc: 0.938277750124\n",
      " loss at iter 400:0.0520  train auc: 0.949365000825  test auc: 0.938277750124\n",
      " loss at iter 401:0.0520  train auc: 0.949419979108  test auc: 0.938277750124\n",
      " loss at iter 402:0.0520  train auc: 0.949419979108  test auc: 0.938277750124\n",
      " loss at iter 403:0.0520  train auc: 0.949419979108  test auc: 0.938277750124\n",
      " loss at iter 404:0.0520  train auc: 0.949419979108  test auc: 0.938277750124\n",
      " loss at iter 405:0.0520  train auc: 0.949474957392  test auc: 0.938277750124\n",
      " loss at iter 406:0.0520  train auc: 0.949529935675  test auc: 0.938277750124\n",
      " loss at iter 407:0.0520  train auc: 0.949529935675  test auc: 0.938277750124\n",
      " loss at iter 408:0.0520  train auc: 0.949529935675  test auc: 0.938277750124\n",
      " loss at iter 409:0.0520  train auc: 0.949529935675  test auc: 0.938277750124\n",
      " loss at iter 410:0.0520  train auc: 0.949529935675  test auc: 0.938277750124\n",
      " loss at iter 411:0.0520  train auc: 0.949529935675  test auc: 0.938277750124\n",
      " loss at iter 412:0.0520  train auc: 0.949529935675  test auc: 0.938277750124\n",
      " loss at iter 413:0.0520  train auc: 0.949529935675  test auc: 0.938277750124\n",
      " loss at iter 414:0.0520  train auc: 0.949529935675  test auc: 0.938277750124\n",
      " loss at iter 415:0.0520  train auc: 0.949529935675  test auc: 0.938277750124\n",
      " loss at iter 416:0.0520  train auc: 0.949529935675  test auc: 0.938775510204\n",
      " loss at iter 417:0.0520  train auc: 0.949529935675  test auc: 0.938775510204\n",
      " loss at iter 418:0.0520  train auc: 0.949529935675  test auc: 0.938775510204\n",
      " loss at iter 419:0.0520  train auc: 0.949529935675  test auc: 0.938775510204\n",
      " loss at iter 420:0.0520  train auc: 0.949529935675  test auc: 0.938775510204\n",
      " loss at iter 421:0.0520  train auc: 0.949529935675  test auc: 0.938775510204\n",
      " loss at iter 422:0.0520  train auc: 0.949529935675  test auc: 0.938775510204\n",
      " loss at iter 423:0.0520  train auc: 0.949584913959  test auc: 0.938775510204\n",
      " loss at iter 424:0.0520  train auc: 0.949584913959  test auc: 0.938775510204\n",
      " loss at iter 425:0.0520  train auc: 0.949639892243  test auc: 0.938775510204\n",
      " loss at iter 426:0.0520  train auc: 0.949639892243  test auc: 0.938775510204\n",
      " loss at iter 427:0.0519  train auc: 0.949639892243  test auc: 0.938775510204\n",
      " loss at iter 428:0.0519  train auc: 0.949639892243  test auc: 0.938775510204\n",
      " loss at iter 429:0.0519  train auc: 0.949639892243  test auc: 0.938775510204\n",
      " loss at iter 430:0.0519  train auc: 0.949639892243  test auc: 0.938775510204\n",
      " loss at iter 431:0.0519  train auc: 0.949639892243  test auc: 0.938775510204\n",
      " loss at iter 432:0.0519  train auc: 0.949639892243  test auc: 0.938775510204\n",
      " loss at iter 433:0.0519  train auc: 0.949694870526  test auc: 0.938775510204\n",
      " loss at iter 434:0.0519  train auc: 0.949694870526  test auc: 0.938775510204\n",
      " loss at iter 435:0.0519  train auc: 0.949694870526  test auc: 0.938775510204\n",
      " loss at iter 436:0.0519  train auc: 0.949694870526  test auc: 0.938775510204\n",
      " loss at iter 437:0.0519  train auc: 0.949694870526  test auc: 0.938775510204\n",
      " loss at iter 438:0.0519  train auc: 0.949694870526  test auc: 0.938775510204\n",
      " loss at iter 439:0.0519  train auc: 0.949694870526  test auc: 0.938775510204\n",
      " loss at iter 440:0.0519  train auc: 0.949694870526  test auc: 0.938775510204\n",
      " loss at iter 441:0.0519  train auc: 0.949694870526  test auc: 0.938775510204\n",
      " loss at iter 442:0.0519  train auc: 0.949694870526  test auc: 0.938775510204\n",
      " loss at iter 443:0.0519  train auc: 0.949694870526  test auc: 0.938775510204\n",
      " loss at iter 444:0.0519  train auc: 0.949694870526  test auc: 0.938775510204\n",
      " loss at iter 445:0.0519  train auc: 0.94974984881  test auc: 0.938775510204\n",
      " loss at iter 446:0.0519  train auc: 0.949804827093  test auc: 0.939273270284\n",
      " loss at iter 447:0.0519  train auc: 0.949804827093  test auc: 0.939273270284\n",
      " loss at iter 448:0.0519  train auc: 0.949804827093  test auc: 0.939273270284\n",
      " loss at iter 449:0.0519  train auc: 0.949804827093  test auc: 0.939273270284\n",
      " loss at iter 450:0.0519  train auc: 0.949804827093  test auc: 0.939273270284\n",
      " loss at iter 451:0.0519  train auc: 0.949804827093  test auc: 0.939273270284\n",
      " loss at iter 452:0.0519  train auc: 0.949804827093  test auc: 0.939273270284\n",
      " loss at iter 453:0.0519  train auc: 0.949859805377  test auc: 0.939273270284\n",
      " loss at iter 454:0.0519  train auc: 0.949859805377  test auc: 0.939273270284\n",
      " loss at iter 455:0.0519  train auc: 0.949859805377  test auc: 0.939273270284\n",
      " loss at iter 456:0.0519  train auc: 0.949859805377  test auc: 0.939273270284\n",
      " loss at iter 457:0.0519  train auc: 0.949859805377  test auc: 0.939273270284\n",
      " loss at iter 458:0.0519  train auc: 0.949859805377  test auc: 0.939273270284\n",
      " loss at iter 459:0.0519  train auc: 0.94991478366  test auc: 0.939273270284\n",
      " loss at iter 460:0.0519  train auc: 0.94991478366  test auc: 0.939273270284\n",
      " loss at iter 461:0.0519  train auc: 0.94991478366  test auc: 0.939771030363\n",
      " loss at iter 462:0.0519  train auc: 0.94991478366  test auc: 0.939771030363\n",
      " loss at iter 463:0.0519  train auc: 0.94991478366  test auc: 0.939771030363\n",
      " loss at iter 464:0.0519  train auc: 0.94991478366  test auc: 0.939771030363\n",
      " loss at iter 465:0.0519  train auc: 0.94991478366  test auc: 0.939771030363\n",
      " loss at iter 466:0.0519  train auc: 0.94991478366  test auc: 0.939771030363\n",
      " loss at iter 467:0.0519  train auc: 0.94991478366  test auc: 0.939771030363\n",
      " loss at iter 468:0.0519  train auc: 0.94991478366  test auc: 0.939771030363\n",
      " loss at iter 469:0.0519  train auc: 0.94991478366  test auc: 0.939771030363\n",
      " loss at iter 470:0.0519  train auc: 0.949969761944  test auc: 0.939771030363\n",
      " loss at iter 471:0.0519  train auc: 0.949969761944  test auc: 0.939771030363\n",
      " loss at iter 472:0.0519  train auc: 0.949969761944  test auc: 0.939771030363\n",
      " loss at iter 473:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 474:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 475:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 476:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 477:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 478:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 479:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 480:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 481:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 482:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 483:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 484:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 485:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 486:0.0519  train auc: 0.950024740228  test auc: 0.939771030363\n",
      " loss at iter 487:0.0519  train auc: 0.950079718511  test auc: 0.940268790443\n",
      " loss at iter 488:0.0519  train auc: 0.950079718511  test auc: 0.940268790443\n",
      " loss at iter 489:0.0519  train auc: 0.950079718511  test auc: 0.940268790443\n",
      " loss at iter 490:0.0519  train auc: 0.950079718511  test auc: 0.940268790443\n",
      " loss at iter 491:0.0519  train auc: 0.950079718511  test auc: 0.940268790443\n",
      " loss at iter 492:0.0519  train auc: 0.950079718511  test auc: 0.940268790443\n",
      " loss at iter 493:0.0519  train auc: 0.950079718511  test auc: 0.940268790443\n",
      " loss at iter 494:0.0519  train auc: 0.950134696795  test auc: 0.940268790443\n",
      " loss at iter 495:0.0519  train auc: 0.950134696795  test auc: 0.940268790443\n",
      " loss at iter 496:0.0519  train auc: 0.950134696795  test auc: 0.940268790443\n",
      " loss at iter 497:0.0519  train auc: 0.950134696795  test auc: 0.940268790443\n",
      " loss at iter 498:0.0519  train auc: 0.950134696795  test auc: 0.940268790443\n",
      " loss at iter 499:0.0519  train auc: 0.950134696795  test auc: 0.940268790443\n",
      " loss at iter 500:0.0519  train auc: 0.950134696795  test auc: 0.940268790443\n",
      " loss at iter 501:0.0519  train auc: 0.950134696795  test auc: 0.940268790443\n",
      " loss at iter 502:0.0519  train auc: 0.950134696795  test auc: 0.940268790443\n",
      " loss at iter 503:0.0519  train auc: 0.950134696795  test auc: 0.940268790443\n",
      " loss at iter 504:0.0519  train auc: 0.950134696795  test auc: 0.940268790443\n",
      " loss at iter 505:0.0519  train auc: 0.950134696795  test auc: 0.940268790443\n",
      " loss at iter 506:0.0519  train auc: 0.950189675078  test auc: 0.940268790443\n",
      " loss at iter 507:0.0519  train auc: 0.950189675078  test auc: 0.940268790443\n",
      " loss at iter 508:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 509:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 510:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 511:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 512:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 513:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 514:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 515:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 516:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 517:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 518:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 519:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 520:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 521:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 522:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 523:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 524:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 525:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 526:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 527:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 528:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 529:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 530:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 531:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 532:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 533:0.0519  train auc: 0.950244653362  test auc: 0.940268790443\n",
      " loss at iter 534:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 535:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 536:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 537:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 538:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 539:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 540:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 541:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 542:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 543:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 544:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 545:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 546:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 547:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 548:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 549:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 550:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 551:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 552:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 553:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 554:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 555:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 556:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 557:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 558:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 559:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 560:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 561:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 562:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 563:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 564:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 565:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 566:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 567:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 568:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 569:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 570:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 571:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 572:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 573:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 574:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 575:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 576:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 577:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 578:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 579:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 580:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 581:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 582:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 583:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 584:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 585:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 586:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 587:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 588:0.0519  train auc: 0.950354609929  test auc: 0.940268790443\n",
      " loss at iter 589:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 590:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 591:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 592:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 593:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 594:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 595:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 596:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 597:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 598:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 599:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 600:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 601:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 602:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 603:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 604:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 605:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 606:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 607:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 608:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 609:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 610:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 611:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 612:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 613:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 614:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 615:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 616:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 617:0.0519  train auc: 0.950354609929  test auc: 0.940766550523\n",
      " loss at iter 618:0.0519  train auc: 0.950354609929  test auc: 0.941264310602\n",
      " loss at iter 619:0.0519  train auc: 0.950354609929  test auc: 0.941264310602\n",
      " loss at iter 620:0.0519  train auc: 0.950354609929  test auc: 0.941264310602\n",
      " loss at iter 621:0.0519  train auc: 0.950354609929  test auc: 0.941264310602\n",
      " loss at iter 622:0.0519  train auc: 0.950354609929  test auc: 0.941264310602\n",
      " loss at iter 623:0.0519  train auc: 0.950354609929  test auc: 0.941264310602\n",
      " loss at iter 624:0.0519  train auc: 0.950409588213  test auc: 0.941264310602\n",
      " loss at iter 625:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 626:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 627:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 628:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 629:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 630:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 631:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 632:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 633:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 634:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 635:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 636:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 637:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 638:0.0519  train auc: 0.950409588213  test auc: 0.941762070682\n",
      " loss at iter 639:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 640:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 641:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 642:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 643:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 644:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 645:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 646:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 647:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 648:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 649:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 650:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 651:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 652:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 653:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 654:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 655:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 656:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 657:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 658:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 659:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 660:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 661:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 662:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 663:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 664:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 665:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 666:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 667:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 668:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 669:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 670:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 671:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 672:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 673:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 674:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 675:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 676:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 677:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 678:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 679:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 680:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 681:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 682:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 683:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 684:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 685:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 686:0.0519  train auc: 0.950464566496  test auc: 0.941762070682\n",
      " loss at iter 687:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 688:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 689:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 690:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 691:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 692:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 693:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 694:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 695:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 696:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 697:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 698:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 699:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 700:0.0519  train auc: 0.95051954478  test auc: 0.941762070682\n",
      " loss at iter 701:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 702:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 703:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 704:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 705:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 706:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 707:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 708:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 709:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 710:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 711:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 712:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 713:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 714:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 715:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 716:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 717:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 718:0.0519  train auc: 0.950574523063  test auc: 0.941762070682\n",
      " loss at iter 719:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 720:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 721:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 722:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 723:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 724:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 725:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 726:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 727:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 728:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 729:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 730:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 731:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 732:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 733:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 734:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 735:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 736:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 737:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 738:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 739:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 740:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 741:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 742:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 743:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 744:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 745:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 746:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 747:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 748:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 749:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 750:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 751:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 752:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 753:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 754:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 755:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 756:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 757:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 758:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 759:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 760:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 761:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 762:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 763:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 764:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 765:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 766:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 767:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 768:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 769:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 770:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 771:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 772:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 773:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 774:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 775:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 776:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 777:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 778:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 779:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 780:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 781:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 782:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 783:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 784:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 785:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 786:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 787:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 788:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 789:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 790:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 791:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 792:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 793:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 794:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 795:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 796:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 797:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 798:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 799:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 800:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 801:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 802:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 803:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 804:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 805:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 806:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 807:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 808:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 809:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 810:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 811:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 812:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 813:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 814:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 815:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 816:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 817:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 818:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 819:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 820:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 821:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 822:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 823:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 824:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 825:0.0519  train auc: 0.950629501347  test auc: 0.941762070682\n",
      " loss at iter 826:0.0519  train auc: 0.950684479631  test auc: 0.941762070682\n",
      " loss at iter 827:0.0519  train auc: 0.950684479631  test auc: 0.942259830762\n",
      " loss at iter 828:0.0519  train auc: 0.950684479631  test auc: 0.942259830762\n",
      " loss at iter 829:0.0519  train auc: 0.950684479631  test auc: 0.942259830762\n",
      " loss at iter 830:0.0519  train auc: 0.950684479631  test auc: 0.942259830762\n",
      " loss at iter 831:0.0519  train auc: 0.950684479631  test auc: 0.942259830762\n",
      " loss at iter 832:0.0519  train auc: 0.950684479631  test auc: 0.942259830762\n",
      " loss at iter 833:0.0519  train auc: 0.950684479631  test auc: 0.942259830762\n",
      " loss at iter 834:0.0519  train auc: 0.950739457914  test auc: 0.942259830762\n",
      " loss at iter 835:0.0519  train auc: 0.950739457914  test auc: 0.942259830762\n",
      " loss at iter 836:0.0519  train auc: 0.950739457914  test auc: 0.942259830762\n",
      " loss at iter 837:0.0519  train auc: 0.950739457914  test auc: 0.942259830762\n",
      " loss at iter 838:0.0519  train auc: 0.950739457914  test auc: 0.942259830762\n",
      " loss at iter 839:0.0519  train auc: 0.950739457914  test auc: 0.942259830762\n",
      " loss at iter 840:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 841:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 842:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 843:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 844:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 845:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 846:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 847:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 848:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 849:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 850:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 851:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 852:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 853:0.0519  train auc: 0.950794436198  test auc: 0.942259830762\n",
      " loss at iter 854:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 855:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 856:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 857:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 858:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 859:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 860:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 861:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 862:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 863:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 864:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 865:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 866:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 867:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 868:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 869:0.0519  train auc: 0.950849414481  test auc: 0.942259830762\n",
      " loss at iter 870:0.0519  train auc: 0.950959371048  test auc: 0.942259830762\n",
      " loss at iter 871:0.0519  train auc: 0.950959371048  test auc: 0.942259830762\n",
      " loss at iter 872:0.0519  train auc: 0.950959371048  test auc: 0.942259830762\n",
      " loss at iter 873:0.0519  train auc: 0.950959371048  test auc: 0.942259830762\n",
      " loss at iter 874:0.0519  train auc: 0.950959371048  test auc: 0.942259830762\n",
      " loss at iter 875:0.0519  train auc: 0.950959371048  test auc: 0.942259830762\n",
      " loss at iter 876:0.0519  train auc: 0.950959371048  test auc: 0.942259830762\n",
      " loss at iter 877:0.0519  train auc: 0.950959371048  test auc: 0.942259830762\n",
      " loss at iter 878:0.0519  train auc: 0.950959371048  test auc: 0.942259830762\n",
      " loss at iter 879:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 880:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 881:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 882:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 883:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 884:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 885:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 886:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 887:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 888:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 889:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 890:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 891:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 892:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 893:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 894:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 895:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 896:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 897:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 898:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 899:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 900:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 901:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 902:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 903:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 904:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 905:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 906:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 907:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 908:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 909:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 910:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 911:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 912:0.0519  train auc: 0.951014349332  test auc: 0.942259830762\n",
      " loss at iter 913:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 914:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 915:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 916:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 917:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 918:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 919:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 920:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 921:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 922:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 923:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 924:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 925:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 926:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 927:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 928:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 929:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 930:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 931:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 932:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 933:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 934:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 935:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 936:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 937:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 938:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 939:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 940:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 941:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 942:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 943:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 944:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 945:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 946:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 947:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 948:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 949:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 950:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 951:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 952:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 953:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 954:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 955:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 956:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 957:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 958:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 959:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 960:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 961:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 962:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 963:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 964:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 965:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 966:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 967:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 968:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 969:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 970:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 971:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 972:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 973:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 974:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 975:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 976:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 977:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 978:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 979:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 980:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 981:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 982:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 983:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 984:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 985:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 986:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 987:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 988:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 989:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 990:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 991:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 992:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 993:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 994:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 995:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 996:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 997:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 998:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      " loss at iter 999:0.0519  train auc: 0.951014349332  test auc: 0.942757590841\n",
      "resulting weights:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x111eeab50>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATAAAAD8CAYAAADwpviIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwCKitZLAIqK1/h+PWlctJZr8QAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11183bbd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for i in range(1000):\n",
    "    loss_i = train_function(X_train,y_train)\n",
    "    print ' loss at iter %i:%.4f' % (i, loss_i),\n",
    "    print ' train auc:', roc_auc_score(y_train, predict_function(X_train)),\n",
    "    print ' test auc:', roc_auc_score(y_test, predict_function(X_test))\n",
    "    \n",
    "print (\"resulting weights:\")\n",
    "plt.imshow(W.get_value().reshape(8,-1))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 align=\"center\">Lasagne</h1>\n",
    "\n",
    "* lasagne - это библиотека для написания нейронок произвольной формы на theano\n",
    "* В качестве демо-задачи выберем то же распознавание чисел, но на большем масштабе задачи, картинки 28x28, 10 цифр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz\n",
      "Downloading train-labels-idx1-ubyte.gz\n",
      "Downloading t10k-images-idx3-ubyte.gz\n",
      "Downloading t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "# print 'X размера', X_train.shape, 'y размера', y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAACqCAYAAAA6El8nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAEAKMAkEAAAAAACQAv8f8nsVTkt/GYcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e544dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=7, figsize=(20, 20))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(X_train[i, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Давайте посмотрим на DenseLayer в lasagne\n",
    "- http://lasagne.readthedocs.io/en/latest/modules/layers/dense.html\n",
    "- https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/dense.py#L16-L124 \n",
    "- Весь содаржательный код тут https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/dense.py#L121 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named lasagne",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-68867e3a6986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonlinearities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named lasagne"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "from theano import tensor as T\n",
    "from lasagne.nonlinearities import softmax\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lasagne import init\n",
    "\n",
    "X, y = T.tensor4('X'), T.vector('y', 'int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Так задаётся архитектура нейронки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lasagne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-46148441f99a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#входной слой (вспомогательный)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# init.Constant() -- плохо\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2DLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# сверточный слой\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lasagne' is not defined"
     ]
    }
   ],
   "source": [
    "#входной слой (вспомогательный)\n",
    "net = lasagne.layers.InputLayer(shape=(None, 1, 28, 28), input_var=X)\n",
    "\n",
    "# init.Constant() -- плохо\n",
    "net = lasagne.layers.Conv2DLayer(net, 15, 3, pad='valid') # сверточный слой\n",
    "net = lasagne.layers.Conv2DLayer(net, 10,  2, pad='full')  # сверточный слой\n",
    "\n",
    "net = lasagne.layers.DenseLayer(net, num_units=500) # полносвязный слой\n",
    "net = lasagne.layers.DropoutLayer(net, 0.5)         # регуляризатор\n",
    "net = lasagne.layers.DenseLayer(net, num_units=200) # полносвязный слой\n",
    "\n",
    "# плохо\n",
    "# net = lasagne.layers.DenseLayer(net, num_units=10)  # полносвязный слой\n",
    "net = lasagne.layers.DenseLayer(net, num_units=10, nonlinearity=lasagne.nonlinearities.softmax)  # полносвязный слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lasagne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-50c4e86b6451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#предсказание нейронки (theano-преобразование)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lasagne' is not defined"
     ]
    }
   ],
   "source": [
    "#предсказание нейронки (theano-преобразование)\n",
    "y_predicted = lasagne.layers.get_output(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lasagne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5f2f6cba73e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#все веса нейронки (shared-переменные)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# all_weights = lasagne.layers.get_all_params(net)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mall_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lasagne' is not defined"
     ]
    }
   ],
   "source": [
    "#все веса нейронки (shared-переменные)\n",
    "# all_weights = lasagne.layers.get_all_params(net)\n",
    "all_weights = lasagne.layers.get_all_params(net, trainable=True)\n",
    "print all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lasagne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-095a7be667c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#функция ошибки и точности будет прямо внутри\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjectives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjectives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lasagne' is not defined"
     ]
    }
   ],
   "source": [
    "#функция ошибки и точности будет прямо внутри\n",
    "loss = lasagne.objectives.categorical_cross_entropy(y_predicted, y).mean()\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#сразу посчитать словарь обновлённых значений с шагом по градиенту, как раньше\n",
    "updates = lasagne.updates.momentum(loss, all_weights, learning_rate=0.01, momentum=0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#функция, делает updates и возвращащет значение функции потерь и точности\n",
    "train_fun = theano.function([X, y], [loss, accuracy], updates=updates)\n",
    "accuracy_fun = theano.function([X, y], accuracy) # точность без обновления весов, для теста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Процесс обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from mnist import iterate_minibatches\n",
    "\n",
    "num_epochs = 10 #количество проходов по данным\n",
    "batch_size = 100 #размер мини-батча\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err, train_acc, train_batches = 0, 0, 0\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc, val_batches = 0, 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print('Epoch %s of %s took' % (epoch + 1, num_epochs))\n",
    "    print('\\t training loss:\\t\\t %.5f' % (train_err / train_batches))\n",
    "    print('\\t train accuracy:\\t %s' % (train_acc / train_batches * 100))\n",
    "    print('\\t validation accuracy:\\t %s' % (val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results: \\n test accuracy:\\t\\t{:.2f} %\".format(test_acc / test_batches * 100))чы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# не забывайте оставлять отзывы \n",
    "# о лекции https://goo.gl/gMeYNL о семинаре https://goo.gl/5hlPD0 :)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
